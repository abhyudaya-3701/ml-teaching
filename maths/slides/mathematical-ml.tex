\documentclass{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/notation/notation}

\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}
\resetcounteronoverlays{saveenumi}

\title{Maths for ML}
\date{\today}
\author{Nipun Batra and teaching staff}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle

\begin{frame}{Maths for ML}
\cleanenumerate
{
    \item Given a vector of $\vecMat{\epsilon}$, we can calculate $\sum \epsilon_{i}^{2}$ using $\dotprod{\epsilon}{\epsilon}$
    \onslide<2->{
        $$
        \vecMat{\epsilon} = \begin{bmatrix}
        \epsilon_{1}   \\
        \epsilon_{2}   \\
        \dots \\
        \epsilon_{N}
        \end{bmatrix}_{N \times 1}   
        $$
    }
    \onslide<3->{
        $$
        \vecMat{\epsilon}\transpose = \begin{bmatrix}
        \epsilon_{1}, 
        \epsilon_{2},  
        \dots, 
        \epsilon_{N}
        \end{bmatrix}_{1 \times N}   
        $$
    }
    \onslide<4->{
        $$\dotprod{\epsilon}{\epsilon} = \sum \epsilon_{i}^{2}$$
            \seti
    }
}
\end{frame}

\begin{frame}{Maths for ML}
\cleanenumerate
{
    \conti
    \item $$
    (\vecMat{A}\vecMat{B})\transpose = \vecMat{B}\transpose\vecMat{A}\transpose    
    $$
    \item 
    For a scalar s
    $$
    s = s^{T}    
    $$
    \seti
}
\end{frame}


\begin{frame}{Maths for ML}
\cleanenumerate
{
    \conti
    \item Derivative of a scalar $s$ wrt a vector $\param$
    \seti
}
 
    $$
    \param = \begin{bmatrix}
    \theta_{1}\\
    \theta_{2}\\
    \vdots\\
    \theta_{N}
    \end{bmatrix}   
    $$
    
    \onslide<2->{
        $$
        \frac{\partial s}{\partial \param} =
        \begin{bmatrix}
            \frac{\partial s}{\partial \theta_{1}}\\
            \frac{\partial s}{\partial \theta_{2}}\\
            \vdots\\
            \frac{\partial s}{\partial \theta_{N}}\\
        \end{bmatrix}
        $$
    }
\end{frame}


\begin{frame}{Linear Functions: Row Vector Times Column Vector}
\begin{definitionbox}{Setup}
\textbf{Configuration:}
\cleanitemize
{
    \item $\vecMat{A} \transpose$ is a row vector ($1 \times n$ matrix)
    \item $\param$ is a column vector ($n \times 1$ matrix) 
    \item $\dotprod{A}{\param}$ produces a scalar
}
\end{definitionbox}

\onslide<4->{
    \begin{examplebox}{Concrete Example}
    $$\param = \begin{bmatrix}
    \theta_{1}\\
    \theta_{2}
    \end{bmatrix}_{2\times 1}, \quad \vecMat{A}\transpose = \begin{bmatrix}
    A_{1} & A_{2}
    \end{bmatrix}_{1 \times 2}$$
    \end{examplebox}
}
\end{frame}

\begin{frame}{Linear Functions: Row Vector Times Column Vector}
\begin{keypointsbox}{Matrix Multiplication Result}
$$\vecMat{A}\transpose \param = A_{1}\theta_{1} + A_{2}\theta_{2}$$
\onslide<2->{\textbf{This is a scalar!} (Linear combination of parameters)}
\end{keypointsbox}

\onslide<3->{
    \begin{alertbox}{ML Relevance}
    This form appears everywhere in ML:
    \onslide<4->{
        \cleanitemize
        {
            \item Linear regression: $\dotprod{w}{x}$
            \item Neural networks: $\dotprod{w}{h} + b$
            \item Loss functions: $\dotprod{c}{\param}$
        }
    }
    \end{alertbox}
}
\end{frame}

\begin{frame}{Gradient of Linear Function: Key Result}
\begin{keypointsbox}{Computing the Gradient}
\textbf{Goal:} Find $\frac{\partial \vecMat{A}\transpose\param}{\partial \param}$ where $\vecMat{A}\transpose\param = A_{1}\theta_{1} + A_{2}\theta_{2}$
\end{keypointsbox}

\onslide<2->{
    \begin{examplebox}{Step-by-Step Calculation}
    $$\frac{\partial \vecMat{A}\transpose\param}{\partial \param} = \begin{bmatrix}
    \frac{\partial}{\partial \theta_{1}}(A_{1}\theta_{1}+A_{2}\theta_{2}) \\
    \frac{\partial}{\partial \theta_{2}}(A_{1}\theta_{1}+A_{2}\theta_{2})
    \end{bmatrix}$$
    \onslide<3->{
        $$= \begin{bmatrix}
        A_{1} \\ A_{2}
        \end{bmatrix}_{2 \times 1} = \vecMat{A}$$
    }
    \end{examplebox}
}
\end{frame}

\begin{frame}{Gradient of Linear Function: Key Result}
\begin{alertbox}{Fundamental Rule}
$$\boxed{\frac{\partial \vecMat{A}\transpose\param}{\partial \param} = \vecMat{A}}$$
\textbf{This is one of the most important rules in ML optimization!}
\end{alertbox}
\end{frame}


\section{Quadratic Forms and Their Derivatives}

\begin{frame}{Quadratic Forms: Introduction}
\begin{definitionbox}{Quadratic Form Derivative Rule}
\textbf{Key Result:} For matrix $\vecMat{Z}$ of form $\dotprod{X}{X}$:
\onslide<2->{$$\frac{\partial}{\partial \param} (\param\transpose\vecMat{Z}\param) = 2\vecMat{Z}\transpose\param$$}
\end{definitionbox}

\onslide<3->{
    \begin{examplebox}{Understanding $\dotprod{X}{X}$ Matrices}
    \onslide<4->{
        Starting with:
        $$\vecMat{X} = \begin{bmatrix}
        a & b \\
        c & d
        \end{bmatrix}, \quad \vecMat{X}\transpose = \begin{bmatrix}
        a & c \\
        b & d
        \end{bmatrix}$$
    }
    \end{examplebox}
}
\end{frame}

\begin{frame}{Quadratic Forms: Introduction}
\begin{keypointsbox}{Computing $\vecMat{Z} = \dotprod{X}{X}$}
\onslide<2->{
    $$\vecMat{Z} = \dotprod{X}{X} = \begin{bmatrix}
    a^{2}+c^{2} & ab+cd \\
    ab+cd & b^{2}+d^{2}
    \end{bmatrix}_{2\times 2}$$
}
\end{keypointsbox}

\onslide<3->{
    \begin{alertbox}{Symmetric Property}
    \onslide<4->{\textbf{Key Observation:} $Z_{ij} = Z_{ji} \Rightarrow \vecMat{Z}\transpose = \vecMat{Z}$ (symmetric matrix)}
    \end{alertbox}
}
\end{frame}


\begin{frame}{Maths for ML}
Let
\begin{equation*}
\vecMat{Z} = \dotprod{X}{X} =  \begin{bmatrix}
e&f\\
f&g
\end{bmatrix}_{2\times 2}
\end{equation*}
\begin{equation*}
    \param = \begin{bmatrix}
    \theta_{1}\\
    \theta_{2}
    \end{bmatrix}_{2\times 1}
\end{equation*}
    
\onslide<2->{
    \begin{equation*}
    \param \transpose \vecMat{Z} \param=  \begin{bmatrix}
    \theta_1&\theta_2\\
    \end{bmatrix}_{1\times 2} \begin{bmatrix}
    e&f\\
    f&g
    \end{bmatrix}_{2\times 2}\begin{bmatrix}
    \theta_{1}\\
    \theta_{2}
    \end{bmatrix}_{2\times 1}
    \end{equation*}
}
    
\onslide<3->{
    \begin{equation*}
    \param \transpose \vecMat{Z} \param = \begin{bmatrix}
    \theta_1&\theta_2\\
    
    \end{bmatrix}_{1\times 2} \begin{bmatrix}
    e\theta_1+f\theta_2\\
    f\theta_1+g\theta_2
    \end{bmatrix}_{2\times 1}
    \end{equation*}
}
    
\onslide<4->{    
    \begin{equation*}
        \param\transpose\vecMat{Z}\param = e\theta_{1}^{2} + 2f\theta_{1}\theta_{2}+g\theta_{2}^{2} 
    \end{equation*}
}
\onslide<5->{The term $\param\transpose\vecMat{Z}\param$ is a scalar.}
\end{frame}


\begin{frame}{Maths for ML}
    
    \begin{center}
    
        \begin{align*}
             \uncover<+-> {\cfrac{\partial}{\partial \param} \param\transpose\vecMat{Z}\param &= \cfrac{\partial}{\partial \theta} (e\theta_{1}^{2} + 2f\theta_{1}\theta_{2}+g\theta_{2}^{2} )\\}
            \uncover<+-> {&= \begin{bmatrix}\cfrac{\partial}{\partial \theta_{1}} (e\theta_{1}^{2} + 2f\theta_{1}\theta_{2}+g\theta_{2}^{2} )\\
            \cfrac{\partial}{\partial \theta_{2}} (e\theta_{1}^{2} + 2f\theta_{1}\theta_{2}+g\theta_{2}^{2} )
            \end{bmatrix}\\}
            \uncover<+-> {&=\begin{bmatrix}2e\theta_{1}+2f\theta_{2}\\
            2f\theta_{1}+2g\theta_{2}\end{bmatrix}
            =  2\begin{bmatrix}e&f\\f&g\end{bmatrix}\begin{bmatrix}\theta_{1}\\ \theta_{1} \end{bmatrix} \\}
            \uncover<+-> {&=2\vecMat{Z}\param = 2\vecMat{Z}\transpose\param}
    \end{align*}
    \end{center}
\end{frame}

\section{Matrix Rank and Invertibility}

\begin{frame}{Matrix Rank: Fundamental Concept}
\onslide<1->{
    \begin{definitionbox}{What is Matrix Rank?}
    \onslide<2->{\textbf{Rank} = Maximum number of linearly independent rows (or columns)}
    \end{definitionbox}
}

\onslide<3->{
    \begin{keypointsbox}{Two Equivalent Perspectives}
    For an $r \times c$ matrix:
    \cleanitemize{
        \onslide<4->{\item \textbf{Row perspective:} $r$ row vectors, each with $c$ elements}
        \onslide<5->{\item \textbf{Column perspective:} $c$ column vectors, each with $r$ elements}
    }
    \end{keypointsbox}
}
\end{frame}

\begin{frame}{Matrix Rank: Fundamental Concept}

\begin{examplebox}{Maximum Rank Rules}
\cleanitemize{
    \item If $r < c$: Maximum rank = $r$ (\textcolor{blue}{more columns than rows})
    \item If $r > c$: Maximum rank = $c$ (\textcolor{red}{more rows than columns})
    \item If $r = c$: Maximum rank = $r = c$ (\textcolor{teal}{square matrix})
}
\end{examplebox}

\end{frame}

\begin{frame}{Maths for ML: Matrix Rank}
\cleanitemize
{
    \item Given a matrix $\vecMat{A}$:$$
    \left[\begin{array}{lll}
    	{0} & {1} & {2} \\
    	{1} & {2} & {1} \\
    	{2} & {7} & {8}
    \end{array}\right]
    $$
    \item What is the rank?
    \item $r=c=3$. Thus, rank is $<=3$
    \item $\rm Row(3) = 3 \times \rm Row(1) + 2 \times \rm Row(2)$. 
    \item Thus, $\rm Row(3)$ is linearly dependent on $\rm Row(1)$ and $\rm Row(2)$.
    \item rank($\vecMat{A}$)=2
}
\end{frame}


\begin{frame}{Maths for ML: Matrix Rank}
What is the rank of
\begin{equation*}
\vecMat{X} =\left[\begin{array}{llll}
{1} & {2} & {4} & {4} \\
{3} & {4} & {8} & {0}
\end{array}\right]
\end{equation*}
\pause Since $\vecMat{X}$ has fewer rows than columns, its maximum rank is equal to the maximum number of linearly independent rows. And because neither row is linearly dependent on the other row, the matrix has 2 linearly independent rows; so its rank is 2.
\end{frame}

%%%%%%%%%%%%%%%%
\popquiz{
\footnotesize
\textbf{What is the rank of a $3 \times 3$ matrix $\vecMat{A}$ formed by the outer product of two non-zero vectors, $\vecMat{u}$ ($3 \times 1$) and $\vecMat{v}\transpose$ ($1 \times 3$)?
$$\vecMat{A} = \vecMat{u}\vecMat{v}\transpose = \begin{bmatrix} u_1 \\ u_2 \\ u_3 \end{bmatrix}
        \begin{bmatrix} v_1 & v_2 & v_3 \end{bmatrix}$$
}
\begin{enumerate}[A)]
    \item 0
    \item 1
    \item 2
    \item 3
\end{enumerate}}{\footnotesize
{\color{magenta}\textbf{B) 1}}
}


\begin{frame}{Maths for ML: Rank of an Outer Product}
    \begin{keypointsbox}{Matrix Formation}
    First, let's construct the matrix $\vecMat{A} = \vecMat{u}\vecMat{v}\transpose$:
    $$
    \vecMat{A} = \begin{bmatrix} u_1 \\ u_2 \\ u_3 \end{bmatrix}
    \begin{bmatrix} v_1 & v_2 & v_3 \end{bmatrix}
    \uncover<2->{=}
    \uncover<2->{
    \begin{bmatrix}
        u_1v_1 & u_1v_2 & u_1v_3 \\
        u_2v_1 & u_2v_2 & u_2v_3 \\
        u_3v_1 & u_3v_2 & u_3v_3
    \end{bmatrix}
    }
    $$
    \end{keypointsbox}
\end{frame}

\begin{frame}{Maths for ML: Rank of an Outer Product}
\footnotesize
    \cleanitemize
    {
        \item \textbf{Look at the columns:} Each column is just a scalar multiple of the original vector $\vecMat{u}$.
        $$
        \text{Column 1} = v_1\vecMat{u}, \quad \text{Column 2} = v_2\vecMat{u}, \quad \text{Column 3} = v_3\vecMat{u}
        $$
        
        \item \textbf{Look at the rows:} Similarly, each row is a scalar multiple of the original vector $\vecMat{v}\transpose$.
         $$
        \text{Row 1} = u_1\vecMat{v}\transpose, \quad \text{Row 2} = u_2\vecMat{v}\transpose, \quad \text{Row 3} = u_3\vecMat{v}\transpose
        $$
    }

    \onslide<3->{
        \begin{alertbox}{Conclusion}
        Since all rows and columns are linearly dependent on a single vector, the maximum number of linearly independent rows (or columns) is one.
        \vspace{1em}
        Therefore, the rank of the matrix is \textbf{1}.
        \end{alertbox}
    }
\end{frame}
%%%%%%%%%%%%%%%%

\begin{frame}{Maths for ML: Matrix Inverse}
Suppose $\vecMat{A}$ is an $n \times n$ matrix. The inverse of $\vecMat{A}$ is another $n \times n$ matrix, denoted $\vecMat{A}^{-1}$, that satisfies the following conditions.
\[
\vecMat{A}\vecMat{A}^{-1}=\vecMat{A}^{-1} \vecMat{A}=\vecMat{I}_n
\]
where $\vecMat{I}_n$ is the identity matrix.

\end{frame}

\begin{frame}{Maths for ML: Matrix Inverse}
There are two ways to determine whether the inverse of a square matrix exists.

\cleanitemize{
    \item If the rank of an $n \times n$ matrix is less than $n,$ the matrix does not have an inverse.
    \item When the determinant for a square matrix is equal to zero, the inverse for that matrix does not exist.
}
\onslide<3->{A square matrix that has an inverse is said to be nonsingular or invertible; a square matrix that does not have an inverse is said to be singular. \\}
\onslide<4-> {Not every square matrix has an inverse; but if a matrix does have an inverse, it is unique.}
\end{frame}

%%%%%%%%%%%%%%%%%%%

\section{Generalizing Derivatives: Gradients and Jacobians}

\begin{frame}{Derivatives of $\mathbb{R}^n \to \mathbb{R}$: The Gradient}
\footnotesize
\begin{definitionbox}{Recap: Derivative of a Scalar Function}
For a function $f: \mathbb{R}^n \to \mathbb{R}$ that takes a vector $\param \in \mathbb{R}^n$ and returns a scalar, its derivative is the \textbf{gradient}.
\onslide<2->{
    $$
    \grad f(\param) = \frac{\partial f}{\partial \param} =
    \begin{bmatrix}
        \frac{\partial f}{\partial \theta_{1}}\\
        \frac{\partial f}{\partial \theta_{2}}\\
        \vdots\\
        \frac{\partial f}{\partial \theta_{n}}\\
    \end{bmatrix}_{n \times 1}
    $$
}
\onslide<3->{\textbf{Note:} By convention in ML, the gradient is a column vector.}
\end{definitionbox}

\onslide<4->{
    \begin{alertbox}{Geometric Intuition}
    The gradient vector $\nabla f(\param)$ points in the direction of the \textbf{steepest ascent} of the function $f$ at point $\param$. The magnitude $||\nabla f(\param)||$ gives the rate of that increase.
    \end{alertbox}
}
\end{frame}

\begin{frame}{From $\mathbb{R}^n \to \mathbb{R}$ to $\mathbb{R}^n \to \mathbb{R}^m$}
\begin{alertbox}{Handling Vector Outputs}
What if our function takes a vector and also \textit{outputs} a vector?\\
\onslide<2->{
    Let $\vecMat{f}: \mathbb{R}^n \to \mathbb{R}^m$. We can think of $\vecMat{f}$ as a stack of $m$ scalar-valued functions:
    $$
    \vecMat{f}(\param) = \begin{bmatrix}
        f_1(\param) \\
        f_2(\param) \\
        \vdots \\
        f_m(\param)
    \end{bmatrix}_{m \times 1}
    $$
}
\onslide<3->{\textbf{Question:} How do we differentiate $\vecMat{f}$ with respect to $\param$?} \\
\onslide<4->{We need to track how \textbf{every output} changes with respect to \textbf{every input}}.
\end{alertbox}
\end{frame}

\begin{frame}{The Jacobian Matrix}
\begin{definitionbox}{The Derivative of a Vector Function}
The derivative of $\vecMat{f}: \mathbb{R}^n \to \mathbb{R}^m$ is the \textbf{Jacobian matrix} $\vecMat{J}$, an $m \times n$ matrix of all first-order partial derivatives.
\onslide<2->{
    $$
    \vecMat{J} = \frac{\partial \vecMat{f}}{\partial \param} = \begin{bmatrix}
        \frac{\partial f_1}{\partial \theta_1} & \frac{\partial f_1}{\partial \theta_2} & \cdots & \frac{\partial f_1}{\partial \theta_n} \\
        \frac{\partial f_2}{\partial \theta_1} & \frac{\partial f_2}{\partial \theta_2} & \cdots & \frac{\partial f_2}{\partial \theta_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial f_m}{\partial \theta_1} & \frac{\partial f_m}{\partial \theta_2} & \cdots & \frac{\partial f_m}{\partial \theta_n}
    \end{bmatrix}_{m \times n}
    $$
}
\onslide<3->{
    \textbf{Key Structure:} Row $i$ of the Jacobian is the transpose of the gradient of the $i$-th output function, $f_i$.
    $$
    (\vecMat{J})_{[i,:]} = (\nabla f_i(\param))\transpose
    $$
}
\end{definitionbox}
\end{frame}

\begin{frame}{Jacobian: A Concrete Example}
\begin{examplebox}{Let's Compute a Jacobian}
Consider $\vecMat{f}: \mathbb{R}^2 \to \mathbb{R}^2$ with $\param = [\theta_1, \theta_2]\transpose$.
$$
\vecMat{f}(\param) = \begin{bmatrix}
f_1(\theta_1, \theta_2) \\
f_2(\theta_1, \theta_2)
\end{bmatrix} =
\begin{bmatrix}
\theta_1^2 \theta_2 \\
5\theta_1 + \sin(\theta_2)
\end{bmatrix}
$$
\onslide<2->{
    The Jacobian $\vecMat{J}$ will be a $2 \times 2$ matrix.
    $$
    \vecMat{J} = \begin{bmatrix}
        \frac{\partial f_1}{\partial \theta_1} & \frac{\partial f_1}{\partial \theta_2} \\
        \frac{\partial f_2}{\partial \theta_1} & \frac{\partial f_2}{\partial \theta_2}
    \end{bmatrix}
    $$
}
\onslide<3->{
    $$
    \vecMat{J} = \begin{bmatrix}
        2\theta_1 \theta_2 & \theta_1^2 \\
        5 & \cos(\theta_2)
    \end{bmatrix}
    $$
}
\end{examplebox}
\end{frame}


\begin{frame}{Visualizing Functions: Graphs and Level Sets}
\footnotesize
\begin{definitionbox}{The Graph of a Function}
For a function $f: \mathbb{R}^n \to \mathbb{R}$, its \textbf{graph} is the set of all input-output pairs.
$$ \text{Graph}(f) = \{ (\param, f(\param)) \mid \param \in \mathbb{R}^n \} $$
\onslide<2->{
    This graph lives in one higher dimension, $\mathbb{R}^{n+1}$.
    
    \textbf{Example:} For $f: \mathbb{R}^2 \to \mathbb{R}$, the graph is a surface in 3D space.
    
}
\end{definitionbox}

\onslide<3->{
\begin{definitionbox}{Level Sets (Contours)}
A \textbf{level set} (or contour) is the set of all points in the \textit{domain} where the function's output is a constant value, $c$.
$$ \text{Level Set}_c = \{ \param \in \mathbb{R}^n \mid f(\param) = c \} $$
\onslide<4->{
    This set lives in the original domain space, $\mathbb{R}^n$.
}
\end{definitionbox}
}
\end{frame}

\begin{frame}{Surplus - Directional Derivative}
\begin{alertbox}{Why the Gradient is the Steepest Direction}
\begin{itemize}[<*>]
    \item Let's define a line through $\vecMat{x} \in \mathbb{R}^n$ on the function $f:\mathbb{R}^n \to \mathbb{R}$ in direction $\vecMat{v}$ as $\vecMat{c}(t) = \vecMat{x} + t\vecMat{v}$. The rate of change of $f$ along this line is $\frac{d}{dt}f(\vecMat{c}(t))$.
    
    \item Using the chain rule, this derivative is $\grad f(\vecMat{c}(t)) \cdot \vecMat{c}'(t)$. At our point $\vecMat{x}$ (where $t=0$), this becomes $\grad f(\vecMat{x}) \cdot \vecMat{v}$.
        
    \item From geometry, we know $\grad f(\vecMat{x}) \cdot \vecMat{v} = ||\grad f(\vecMat{x})|| \ ||\vecMat{v}|| \cos(\theta)$. Since $||\vecMat{v}||=1$, this value is maximized when $\cos(\theta)=1$.
        
    \item This occurs when $\vecMat{v}$ points in the same direction as $\grad f(\vecMat{x})$. Thus, the gradient points in the direction of steepest ascent.
\end{itemize}
\end{alertbox}
\end{frame}

\end{document}