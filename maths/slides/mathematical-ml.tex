\documentclass{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/notation/notation}

\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}
\resetcounteronoverlays{saveenumi}

\title{Maths for ML}
\date{\today}
\author{Nipun Batra and teaching staff}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle

\begin{frame}{Maths for ML}
\cleanenumerate
{
    \item Given a vector of $\vecMat{\epsilon}$, we can calculate $\sum \epsilon_{i}^{2}$ using $\dotprod{\epsilon}{\epsilon}$
    \onslide<2->{
        $$
        \vecMat{\epsilon} = \begin{bmatrix}
        \epsilon_{1}   \\
        \epsilon_{2}   \\
        \dots \\
        \epsilon_{N}
        \end{bmatrix}_{N \times 1}   
        $$
    }
    \onslide<3->{
        $$
        \vecMat{\epsilon}\transpose = \begin{bmatrix}
        \epsilon_{1}, 
        \epsilon_{2},  
        \dots, 
        \epsilon_{N}
        \end{bmatrix}_{1 \times N}   
        $$
    }
    \onslide<4->{
        $$\dotprod{\epsilon}{\epsilon} = \sum \epsilon_{i}^{2}$$
            \seti
    }
}
\end{frame}

\begin{frame}{Maths for ML}
\cleanenumerate
{
    \conti
    \item $$
    (\vecMat{A}\vecMat{B})\transpose = \vecMat{B}\transpose\vecMat{A}\transpose    
    $$
    \item 
    For a scalar s
    $$
    s = s^{T}    
    $$
    \seti
}
\end{frame}


\begin{frame}{Maths for ML}
\cleanenumerate
{
    \conti
    \item Derivative of a scalar $s$ wrt a vector $\param$
    \seti
}
 
    $$
    \param = \begin{bmatrix}
    \theta_{1}\\
    \theta_{2}\\
    \vdots\\
    \theta_{N}
    \end{bmatrix}   
    $$
    
    \onslide<2->{
        $$
        \frac{\partial s}{\partial \param} =
        \begin{bmatrix}
            \frac{\partial s}{\partial \theta_{1}}\\
            \frac{\partial s}{\partial \theta_{2}}\\
            \vdots\\
            \frac{\partial s}{\partial \theta_{N}}\\
        \end{bmatrix}
        $$
    }
\end{frame}


\begin{frame}{Linear Functions: Row Vector Times Column Vector}
\begin{definitionbox}{Setup}
\textbf{Configuration:}
\cleanitemize
{
    \item $\vecMat{A}$ is a row vector ($1 \times n$ matrix)
    \item $\param$ is a column vector ($n \times 1$ matrix) 
    \item $\dotprod{A}{\param}$ produces a scalar
}
\end{definitionbox}

\onslide<4->{
    \begin{examplebox}{Concrete Example}
    $$\param = \begin{bmatrix}
    \theta_{1}\\
    \theta_{2}
    \end{bmatrix}_{2\times 1}, \quad \vecMat{A} = \begin{bmatrix}
    A_{1} & A_{2}
    \end{bmatrix}_{1 \times 2}$$
    \end{examplebox}
}
\end{frame}

\begin{frame}{Linear Functions: Row Vector Times Column Vector}
\begin{keypointsbox}{Matrix Multiplication Result}
$$\vecMat{A}\param = A_{1}\theta_{1} + A_{2}\theta_{2}$$
\onslide<2->{\textbf{This is a scalar!} (Linear combination of parameters)}
\end{keypointsbox}

\onslide<3->{
    \begin{alertbox}{ML Relevance}
    This form appears everywhere in ML:
    \onslide<4->{
        \cleanitemize
        {
            \item Linear regression: $\dotprod{w}{x}$
            \item Neural networks: $\dotprod{w}{h} + b$
            \item Loss functions: $\dotprod{c}{\param}$
        }
    }
    \end{alertbox}
}
\end{frame}

\begin{frame}{Gradient of Linear Function: Key Result}
\begin{keypointsbox}{Computing the Gradient}\\
\onslide<2->{\textbf{Goal:} Find $\frac{\partial \vecMat{A}\param}{\partial \param}$ where $\vecMat{A}\param = A_{1}\theta_{1} + A_{2}\theta_{2}$}
\end{keypointsbox}

\onslide<3->{
    \begin{examplebox}{Step-by-Step Calculation}
    $$\frac{\partial \vecMat{A}\param}{\partial \param} = \begin{bmatrix}
    \frac{\partial}{\partial \theta_{1}}(A_{1}\theta_{1}+A_{2}\theta_{2}) \\
    \frac{\partial}{\partial \theta_{2}}(A_{1}\theta_{1}+A_{2}\theta_{2})
    \end{bmatrix}$$
    \onslide<4->{
        $$= \begin{bmatrix}
        A_{1} \\ A_{2}
        \end{bmatrix}_{2 \times 1} = \vecMat{A}\transpose$$
    }
    \end{examplebox}
}
\end{frame}

\begin{frame}{Gradient of Linear Function: Key Result}
\begin{alertbox}{Fundamental Rule}
$$\boxed{\frac{\partial \vecMat{A}\param}{\partial \param} = \vecMat{A}\transpose}$$
\onslide<2->{\textbf{This is one of the most important rules in ML optimization!}}
\end{alertbox}
\onslide<3->{
    \begin{definitionbox}{Intuition}
    \textbf{Why $\vecMat{A}\transpose$?} \onslide<4->{Each component of the gradient equals the coefficient of the corresponding parameter in the linear function.}
    \end{definitionbox}
}
\end{frame}


\section{Quadratic Forms and Their Derivatives}

\begin{frame}{Quadratic Forms: Introduction}
\begin{definitionbox}{Quadratic Form Derivative Rule}
\textbf{Key Result:} For matrix $\vecMat{Z}$ of form $\dotprod{X}{X}$:
\onslide<2->{$$\frac{\partial}{\partial \param} (\param\transpose\vecMat{Z}\param) = 2\vecMat{Z}\transpose\param$$}
\end{definitionbox}

\onslide<3->{
    \begin{examplebox}{Understanding $\dotprod{X}{X}$ Matrices}
    \onslide<4->{
        Starting with:
        $$\vecMat{X} = \begin{bmatrix}
        a & b \\
        c & d
        \end{bmatrix}, \quad \vecMat{X}\transpose = \begin{bmatrix}
        a & c \\
        b & d
        \end{bmatrix}$$
    }
    \end{examplebox}
}
\end{frame}

\begin{frame}{Quadratic Forms: Introduction}
\begin{keypointsbox}{Computing $\vecMat{Z} = \dotprod{X}{X}$}
\onslide<2->{
    $$\vecMat{Z} = \dotprod{X}{X} = \begin{bmatrix}
    a^{2}+c^{2} & ab+cd \\
    ab+cd & b^{2}+d^{2}
    \end{bmatrix}_{2\times 2}$$
}
\end{keypointsbox}

\onslide<3->{
    \begin{alertbox}{Symmetric Property}
    \onslide<4->{\textbf{Key Observation:} $Z_{ij} = Z_{ji} \Rightarrow \vecMat{Z}\transpose = \vecMat{Z}$ (symmetric matrix)}
    \end{alertbox}
}
\end{frame}


\begin{frame}{Maths for ML}
Let
\begin{equation*}
\vecMat{Z} = \dotprod{X}{X} =  \begin{bmatrix}
e&f\\
f&g
\end{bmatrix}_{2\times 2}
\end{equation*}
\begin{equation*}
    \param = \begin{bmatrix}
    \theta_{1}\\
    \theta_{2}
    \end{bmatrix}_{2\times 1}
\end{equation*}
    
\onslide<2->{
    \begin{equation*}
    \param \transpose \vecMat{Z} \param=  \begin{bmatrix}
    \theta_1&\theta_2\\
    \end{bmatrix}_{1\times 2} \begin{bmatrix}
    e&f\\
    f&g
    \end{bmatrix}_{2\times 2}\begin{bmatrix}
    \theta_{1}\\
    \theta_{2}
    \end{bmatrix}_{2\times 1}
    \end{equation*}
}
    
\onslide<3->{
    \begin{equation*}
    \param \transpose \vecMat{Z} \param = \begin{bmatrix}
    \theta_1&\theta_2\\
    
    \end{bmatrix}_{1\times 2} \begin{bmatrix}
    e\theta_1+f\theta_2\\
    f\theta_1+g\theta_2
    \end{bmatrix}_{2\times 1}
    \end{equation*}
}
    
\onslide<4->{    
    \begin{equation*}
        \param\transpose\vecMat{Z}\param = e\theta_{1}^{2} + 2f\theta_{1}\theta_{2}+g\theta_{2}^{2} 
    \end{equation*}
}
\onslide<5->{The term $\param\transpose\vecMat{Z}\param$ is a scalar.}
\end{frame}


\begin{frame}{Maths for ML}
    
    \begin{center}
    
        \begin{align*}
             \uncover<+-> {\cfrac{\partial}{\partial \param} \param\transpose\vecMat{Z}\param &= \cfrac{\partial}{\partial \theta} (e\theta_{1}^{2} + 2f\theta_{1}\theta_{2}+g\theta_{2}^{2} )\\}
            \uncover<+-> {&= \begin{bmatrix}\cfrac{\partial}{\partial \theta_{1}} (e\theta_{1}^{2} + 2f\theta_{1}\theta_{2}+g\theta_{2}^{2} )\\
            \cfrac{\partial}{\partial \theta_{2}} (e\theta_{1}^{2} + 2f\theta_{1}\theta_{2}+g\theta_{2}^{2} )
            \end{bmatrix}\\}
            \uncover<+-> {&=\begin{bmatrix}2e\theta_{1}+2f\theta_{2}\\
            2f\theta_{1}+2g\theta_{2}\end{bmatrix}
            =  2\begin{bmatrix}e&f\\f&g\end{bmatrix}\begin{bmatrix}\theta_{1}\\ \theta_{1} \end{bmatrix} \\}
            \uncover<+-> {&=2\vecMat{Z}\param = 2\vecMat{Z}\transpose\param}
    \end{align*}
    \end{center}
\end{frame}

\section{Matrix Rank and Invertibility}

\begin{frame}{Matrix Rank: Fundamental Concept}
\onslide<1->{
    \begin{definitionbox}{What is Matrix Rank?}
    \onslide<2->{\textbf{Rank} = Maximum number of linearly independent rows (or columns)}
    \end{definitionbox}
}

\onslide<3->{
    \begin{keypointsbox}{Two Equivalent Perspectives}
    For an $r \times c$ matrix:
    \cleanitemize{
        \onslide<4->{\item \textbf{Row perspective:} $r$ row vectors, each with $c$ elements}
        \onslide<5->{\item \textbf{Column perspective:} $c$ column vectors, each with $r$ elements}
    }
    \end{keypointsbox}
}
\end{frame}

\begin{frame}{Matrix Rank: Fundamental Concept}

\begin{examplebox}{Maximum Rank Rules}
\cleanitemize{
    \item If $r < c$: Maximum rank = $r$ (\textcolor{blue}{more columns than rows})
    \item If $r > c$: Maximum rank = $c$ (\textcolor{red}{more rows than columns})
    \item If $r = c$: Maximum rank = $r = c$ (\textcolor{teal}{square matrix})
}
\end{examplebox}

\onslide<4->{
    \begin{alertbox}{ML Relevance}
    \textbf{Why rank matters:}
    \cleanitemize{
        \item Determines if matrix is invertible
        \item Affects uniqueness of solutions
        \item Critical for understanding overfitting
    }
    \end{alertbox}
}
\end{frame}

\begin{frame}{Maths for ML: Matrix Rank}
\begin{itemize}[<+-> ] 
\item Given a matrix $\vecMat{A}$:$$
\left[\begin{array}{lll}
	{0} & {1} & {2} \\
	{1} & {2} & {1} \\
	{2} & {7} & {8}
\end{array}\right]
$$
\item What is the rank?
\item $r=c=3$. Thus, rank is $<=3$
\item Row 3 can be written as: 3 times Row 1 + 2 times Row 1. Thus, Row 3 is linearly dependent on Row 1 and 2. Thus, rank($\vecMat{A}$)=2
\end{itemize}
\end{frame}


\begin{frame}{Maths for ML: Matrix Rank}
What is the rank of
\begin{equation*}
\vecMat{X} =\left[\begin{array}{llll}
{1} & {2} & {4} & {4} \\
{3} & {4} & {8} & {0}
\end{array}\right]
\end{equation*}
\pause Since $\vecMat{X}$ has fewer rows than columns, its maximum rank is equal to the maximum number of linearly independent rows. And because neither row is linearly dependent on the other row, the matrix has 2 linearly independent rows; so its rank is 2.
\end{frame}

\begin{frame}{Maths for ML: Matrix Inverse}
Suppose $\vecMat{A}$ is an $n \times n$ matrix. The inverse of $\vecMat{A}$ is another $n \times n$ matrix, denoted $\vecMat{A}^{-1}$, that satisfies the following conditions.
\[
\vecMat{A}\vecMat{A}^{-1}=\vecMat{A}^{-1} \vecMat{A}=\vecMat{I}_n
\]
where $\vecMat{I}_n$ is the identity matrix.

\pause  Below, with an example, we illustrate the relationship between a matrix and its inverse.

\pause \[
\begin{array}{l}
{\left[\begin{array}{cc}
	{2} & {1} \\
	{3} & {4}
	\end{array}\right]\left[\begin{array}{cc}
	{0.8} & {-0.2} \\
	{-0.6} & {0.4}
	\end{array}\right]=\left[\begin{array}{ll}
	{1} & {0} \\
	{0} & {1}
	\end{array}\right]} \\ \\
{\left[\begin{array}{cc}
	{0.8} & {-0.2} \\
	{-0.6} & {0.2}
	\end{array}\right]\left[\begin{array}{ll}
	{2} & {1} \\
	{3} & {4}
	\end{array}\right]=\left[\begin{array}{ll}
	{1} & {0} \\
	{0} & {1}
	\end{array}\right]}
\end{array}
\]

\end{frame}

\begin{frame}{Maths for ML: Matrix Inverse}
There are two ways to determine whether the inverse of a square matrix exists.

\cleanitemize{
    \item If the rank of an $n \times n$ matrix is less than $n,$ the matrix does not have an inverse.
    \item When the determinant for a square matrix is equal to zero, the inverse for that matrix does not exist.
}
\onslide<3->{A square matrix that has an inverse is said to be nonsingular or invertible; a square matrix that does not have an inverse is said to be singular. \\}
\onslide<4-> {Not every square matrix has an inverse; but if a matrix does have an inverse, it is unique.}
\end{frame}

\end{document}