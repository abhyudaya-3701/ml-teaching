\documentclass{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/styles/conventions}
\usepackage{../../shared/notation/notation}

\mathtoolsset{showonlyrefs}  
\title{Multivariate Normal Distribution I}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle

\begin{frame}{Univariate Normal Distribution}

The probability density of univariate Gaussian is given as: $$f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$
	
also, given as 
$$f(x)\sim \distribReal{N}{\mu}{\sigma^2}$$

with mean $\mu \in \Real$ and variance $\sigma^2 >0$ 

\end{frame}

\begin{frame}{Why is the denominator the way it is?}
\onslide<1->{Let the normalizing constant be $c$ and let $g(x) = e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$.}

\begin{gather}
\visible<2->{1 = \int_{-\infty}^{\infty} c \cdot g(x) dx\\}
\visible<3->{1 = \int_{-\infty}^{\infty} ce^{-\frac{(x - u)^2}{2\sigma^2}} dx}
\end{gather}
\visible<4->{Let's substitute $\frac{x - u}{\sqrt{2}\sigma}$ with $t$.}
\begin{gather}
\visible<5->{1 = \int_{-\infty}^{\infty} ce^{-t^2} dt \times \sqrt{2}\sigma\\}
\visible<6->{1 = \sqrt{2}\sigma c \times 2\int_{0}^{\infty} e^{-t^2} dt}
\end{gather}
\end{frame}

\begin{frame}{Why is the denominator the way it is?}
	$$ \frac{2}{\sqrt{\pi}}\int_{0}^{\infty} e^{-t^2} dt $$
	The above expression is called \textbf{error function} and is it's value is denoted by $erf(t)$. In our case, we want $erf(\infty)$ which is equal to 1.
	
	\begin{gather}
	\visible<2->{1 = \sqrt{2\pi}\sigma c \times \frac{2}{\sqrt{\pi}}\int_{0}^{\infty} e^{-t^2} dt\\}
	\visible<3->{1 = \sqrt{2\pi}\sigma c \times 1\\}
	\visible<4->{\frac{1}{\sqrt{2\pi}\sigma} = c}
	\end{gather}
\end{frame}

\begin{frame}{Univariate Normal Distribution}
	\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{fig1} 
\end{frame}

\begin{frame}{Univariate Normal Distribution}
	\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{fig2}
\end{frame}

\begin{frame}{Univariate Normal Distribution}
	\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{fig3}
\end{frame}

\begin{frame}{Bivariate Normal Distribution}
Bivariate normal distribution of two-dimensional random vector $\mX =\begin{bmatrix}
X_{1} \\
X_{2} \\
\end{bmatrix}
$

\begin{gather}
	\mX = \begin{pmatrix}
	X_1 \\
	X_2
	\end{pmatrix} \sim \distribReal{N}{\vmu}{\mSigma}
\end{gather}

where, mean vector $\vmu =\begin{bmatrix}
\mu_{1} \\
\mu_{2} \\
\end{bmatrix}
=\begin{bmatrix}
\Exp[X_{1}] \\
\Exp[X_{2}] \\
\end{bmatrix}
$

and, covariance matrix $\mSigma$
$$\mSigma_{i,j} \defined \Exp [(X_i - \mu_i)( X_j - \mu_j)] = \Cov[X_i, X_j] $$
\end{frame}

\begin{frame}{Bivariate Normal Distribution}
	
	\visible<1->{\textbf{Question}: What is $\Cov(\mX, \mX)$?}
	
	\visible<2->{\textbf{Answer}: $\Var(\mX)$ = $\Cov(\mX, \mX)$ =  $\Exp[(\mX - \Exp[\mX])]^2$}
	
	\visible<2->{In the case of univariate normal, $\Var(X)$ is written as $\sigma^2$}
	
	\visible<3->{\textbf{Question}: What is the relation between $\mSigma_{i, j}$ and $\mSigma_{j, i}$?}
	
	\visible<4->{\textbf{Answer}: They are the same!}
	
	\visible<5->{\textbf{Question}: What can we say about the covariance matrix $\mSigma$?}
	
	\visible<6->{\textbf{Answer}: It is symmetric. Thus $\mSigma = \mSigma \tp$}
	
\end{frame}

\begin{frame}{Correlation and Covariance}
If $X$ and $Y$ are two random variables, with means (expected values) $\mu_X$ and $\mu_Y$ and standard deviations $\sigma_X$ and $\sigma_Y$, respectively, then their covariance and correlation are as follows:

$$\Cov_{XY} = \sigma_{XY} = \Exp[(X-\mu_X)\,(Y-\mu_Y)] $$

\visible<2->{$$	\Corr_{XY} = \rho_{XY} = \Exp[(X-\mu_X)\,(Y-\mu_Y)]/(\sigma_X \sigma_Y)
$$}
\visible<3->{so that
$$
\rho_{XY} = \sigma_{XY} / (\sigma_X \sigma_Y) $$

where $\Exp$ is the expected value operator. }
\end{frame}

\begin{frame}{PDF of bivariate normal distribution}

We might have seen that 

$$f_{\mX}(X_1, X_2) = \frac{\exp(\frac{-1}{2}(\mX-\vmu)\tp \mSigma\inv(\mX-\vmu))}{2\pi \det(\mSigma)^\frac{1}{2}}$$

How do we get such a weird looking formula?!

\end{frame}

\begin{frame}{PDF of bivariate normal with no cross-correlation}

Let us assume no correlation between $X_1$ and $X_2$.

We have $\mSigma = \begin{bmatrix}
\sigma_1^2 & 0 \\
0 & \sigma_2^2 \\
\end{bmatrix}$

We have $f_{\mX}(X_1, X_2) = f_{\mX}(X_1)f_{\mX}(X_2)$

$$=\frac{1}{\sigma_1 \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{X_1-\mu_1}{\sigma_1}\right)^2} \times \frac{1}{\sigma_2 \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{X_2-\mu_2}{\sigma_2}\right)^2}$$

$$= \frac{1}{\sigma_1 \sigma_2 2\pi } e^{-\frac{1}{2}\left\{\left(\frac{X_1-\mu_1}{\sigma_1}\right)^2 + \left(\frac{X_2-\mu_2}{\sigma_2}\right)^2 \right\}}  $$
\end{frame}

\begin{frame}{PDF of bivariate normal with no cross-correlation}
Let us consider only the exponential part for now

$ Q =  \left(\frac{X_1-\mu_1}{\sigma_1}\right)^2 + \left(\frac{X_2-\mu_2}{\sigma_2}\right)^2 $

\textbf{Question}: Can you write Q in the form of vectors $\mX$ and $\vmu$?

$$
 = \begin{bmatrix}
	X_1 - \mu_1 &
	X_2 - \mu_2 \\
\end{bmatrix}_{1\times2}  g(\mSigma)_{2\times2} \begin{bmatrix}
X_1 - \mu_1 \\
X_2 - \mu_2 \\
\end{bmatrix}_{2\times1}
$$

Here $g(\mSigma)$ is a matrix function of $\mSigma$ that will result in $\sigma_1^2$ like terms in the denominator; also there is no cross-terms indicating zeros in right diagonal!
\footnotesize
$
g(\mSigma) = \begin{bmatrix}
 1/\sigma_1^2& 0  \\
 0 &  1/\sigma_2^2 \\
\end{bmatrix}_{2\times2} = \frac{1}{\sigma_1^2 \sigma_2^2}\begin{bmatrix}
{\sigma_2^2}& 0  \\
0 &  {\sigma_1^2}   \\ 
\end{bmatrix}_{2\times2} = \frac{1}{\det(\mSigma)} \adj(\mSigma) = \mSigma\inv$
$
\end{frame}


\begin{frame}{PDF of bivariate normal with no cross-correlation}
Let us consider the normalizing constant part now.
\begin{align*}
M &= \frac{1}{2\pi\ \sigma_1\ \sigma_2}\\
&= \frac{1}{2\pi \times \det(\mSigma)^{\frac{1}{2}}}
\end{align*}
\end{frame}

\begin{frame}{Bivariate Gaussian samples with cross-correlation  $\neq$ 0}
	\begin{center}
		\includegraphics[width=\linewidth,height=\textheight - 10pt,keepaspectratio]{cross-non-zero}
	\end{center}
\end{frame}

\begin{frame}{Bivariate Gaussian samples with cross-correlation  = 0}
	\begin{center}
		\includegraphics[width=\linewidth,height=\textheight - 10pt,keepaspectratio]{cross-0}
	\end{center}
\end{frame}



\begin{frame}{Intuition for Multivariate Gaussian}
	
	Let us assume no correlation between the elements of $\mX$. This means $\mSigma$ is a diagonal matrix.
	
	We have $\mSigma = \begin{bmatrix} 
	\sigma_{1}^2 & & \vzero\\
	 & \ddots &\\
	\vzero &  & \sigma_n^2 
	\end{bmatrix}
	$ 
	
	And,
	\begin{gather}
		\Prob(\mX ; \vmu, \mSigma)=\frac{1}{(2 \pi)^{\frac{n}{2}}\det(\mSigma)^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(\mX-\vmu)\tp \mSigma\inv(\mX-\vmu)\right)
	\end{gather}
	
	As seen in the case for univariate Gaussians, we can write the following for the multivariate case,
	
	We have $f_{\mX}(X_1, \cdots, X_n) = f_{\mX}(X_1)\times \cdots \times f_{\mX}(X_n)$
\end{frame}

\begin{frame}{Intuition for Multivariate Gaussian}
	Now,
	$$=\frac{1}{\sigma_1 \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{X_1-\mu_1}{\sigma_1}\right)^2} \times \cdots \times \frac{1}{\sigma_n \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{X_n-\mu_n}{\sigma_n}\right)^2}$$
	
	$$= \frac{1}{\sigma_1 \cdots \sigma_n (2\pi)^{\frac{n}{2}} } \exp\left(-\frac{1}{2}\left\{\left(\frac{X_1-\mu_1}{\sigma_1}\right)^2 + \cdots + \left(\frac{X_n-\mu_n}{\sigma_n}\right)^2 \right\}\right)  $$
	
	Taking all $\sqrt{2\pi}$ together, we get $(2\pi)^{\frac{n}{2}}$.
	
	Similarly, taking all $\sigma$ together, we get $\sigma_1 \cdots \sigma_n$. Which can be written as $\det(\mSigma)^{\frac{1}{2}}$, given the determinant of a digonal matrix is the multiplication of its diagonal elements.
\end{frame}

\begin{frame}
	\footnotesize
	Now, let us remove the assumption of no covariance among the elements of $\mX$
	
	\textbf{Main idea}: A correlated Gaussian is a rotated independent Gaussian\footnote{Neil Lawrence GPSS 2016}
	
	
	Rotate input space using rotation matrix $\mR$.
	
		\begin{gather}
		\Prob(\mX ; \vmu, \mSigma)=\frac{1}{(2 \pi)^{\frac{n}{2}}\det(\mSigma)^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(\mR\tp\mX-\mR\tp\vmu)\tp \mSigma\inv(\mR\tp\mX-\mR\tp\vmu)\right)
	\end{gather}

	
\begin{gather}
	\Prob(\mX ; \vmu, \mSigma)=\frac{1}{(2 \pi)^{\frac{n}{2}}\det(\mSigma)^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(\mX-\vmu)\tp \mR\mSigma\invR\tp(\mX-\vmu)\right)
\end{gather}
	
	
\end{frame}

\begin{frame}
	
	$\mC = \mR\mSigma\invR\tp$
\begin{gather}
	\Prob(\mX ; \vmu, \mSigma)=\frac{1}{(2 \pi)^{\frac{n}{2}}\det(\mC)^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(\mX-\vmu)\tp \mC\inv(\mX-\vmu)\right)
\end{gather}
\end{frame}

\end{document}
