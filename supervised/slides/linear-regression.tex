\documentclass{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/styles/conventions}

\newcommand{\vecuvec}[2] %start point, end point (of vector)
{   \VECTORSUB(#2)(#1)(\sola,\solb,\solc)
	\UNITVECTOR(\sola, \solb, \solc)(\sola,\solb,\solc)
	%arrow in blue
	\draw[->,thick,blue] (#1) -- (#2); 
	%corresponding unit-vector in red:
	\edef\temp{\noexpand\draw[->, thick,red] (#1) -- ($(#1)+(\sola,\solb,\solc)$);}
	\temp
}

\title{Linear Regression}
\date{\today}
\author{Nipun Batra and the teaching staff}
\institute{IIT Gandhinagar}
\begin{document}
\maketitle

\begin{frame}{Table of Contents}
\tableofcontents
\end{frame}

\section{Setup}
  
% \section{Linear Regression}

\begin{frame}{Linear Regression}
\begin{itemize}
	
	
	\item<+-> Output is continuous in nature.
	\item<+-> Examples of linear systems:
	\begin{itemize}
		\item<+-> $F=ma$
		\item<+-> $v=u+at$
	\end{itemize}
	
\end{itemize}
\end{frame}

\begin{frame}{Task at hand}
\begin{itemize}

\item TASK: Predict Weight = f(height)
\end{itemize}
\begin{center}
    

\begin{tabular}{ |c|c|c|c| } 
\hline
 Height & Weight \\
\hline
3 & 29 \\ 
4 & 35 \\ 
5 & 39\\
2 & 20\\
6 & 41\\
\hline
\hline
7 & ?\\
8 & ?\\
1 &? \\
\hline
\end{tabular}

\end{center}
The first part of the dataset is the training points. The latter ones are testing points.	

\end{frame}


\begin{frame}{Scatter Plot}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$height~(ft)$,
ylabel=$weight~(kg)$,
xmin=0,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside
]

\addplot[only marks, mark=*]
coordinates{ % plot 1 dataset
	(6, 80)
	(5.5, 80)
	(6, 70)
	(5, 40)
	(5, 60)
	(4, 30)
	(4, 45)
	(4, 20)
	(6, 120)
	(2, 15)
	(1, 5)
	(0.5, 4)
	(3, 20)
	(6, 62)
	(5, 57)
	

	% more points...
}; 
\node[label={180:{Outlier}},circle,fill,inner sep=2pt] at (axis cs:6,120) {};
\node[label={-90:{Outlier}},circle,fill,inner sep=2pt] at (axis cs:4,20) {};

% plot 1 legend entry


\end{axis}

\end{tikzpicture}


\end{frame}


\begin{frame}{Matrix representation of the expression}


\begin{itemize}
\item $weight_{1} \approx \theta_{0} + \theta_{1} \cdot height_{1}$

\item $weight_{2} \approx \theta_{0} + \theta_{1} \cdot height_{2}$

\item $weight_{N} \approx \theta_{0} + \theta_{1} \cdot height_{N}$

\end{itemize}
\begin{center}
\pause	\begin{tcolorbox}
		$weight_{i} \approx \theta_{0} + \theta_{1} \cdot height_{i}$
	\end{tcolorbox}
\end{center}
\end{frame}



\begin{frame}{Matrix representation of the expression}



\[\begin{bmatrix}
    weight_{1}   \\
    weight_{2}   \\
    \dots \\
    weight_{N}
\end{bmatrix}
= \begin{bmatrix}
    1& height_{1}   \\
    1& height_{2}   \\
    \dots & \dots  \\
    1& height_{N}   \\
\end{bmatrix}
\begin{bmatrix}
    \theta_{0} \\
    \theta_{1}
\end{bmatrix}\] \\

\pause \[\yhat_{n \times 1} = \mX_{n \times d} \vtheta_{d \times 1} \]


%W(N \times 1); X(N\times 2); \theta(2 \times 1);


\pause \begin{itemize}
    \item<+-> $\theta_{0}$ - Bias Term/Intercept Term
    \item<+-> $\theta_{1}$ - Slope
\end{itemize}
\end{frame}



\begin{frame}{Extension to multiple dimensions}

\begin{itemize}[<+->]
    \item In the previous example y = f(x), where x is one-dimensional
    \item Now consider examples in multiple dimensions
    \item Example: Predict the water demand of the IITGN campus
    \item \small{Mathematical representation:
        \begin{center}
            \begin{tcolorbox}
                Demand = f(\# occupants, Temperature)
            \end{tcolorbox}
        \end{center}}
    \item \small{Linear form:
        \begin{center}
            \begin{tcolorbox}
                Demand = Base Demand + $K_{1}$ * \# occupants + $K_{2}$ * Temperature
            \end{tcolorbox}
        \end{center}}
\end{itemize}

\end{frame}

\begin{frame}{Intuition}
    We hope to: 
    \begin{itemize}
        \item Learn $f$: $Demand$ = $f(\# occupants, Temperature)$
        \item From training dataset
        \item To predict the condition for the testing set
    \end{itemize}
\end{frame}


\begin{frame}{Linear Relationship}
    We have 
    \begin{itemize}[<+->]
    	\item $x_i = \begin{bmatrix}
    	Temperature_i\\\#Occupants_i
    	\end{bmatrix}$
        \item Estimated demand for $i^{th}$ sample is  $\hat{demand_{i}} = \theta_{0} + \theta_{1} Temperature_i + \theta_{2} Occupants_i$
        \item $\hat{demand_{i}} = x_{i}'^{T} \vtheta$
        \item where $\vtheta = \begin{bmatrix}\theta_0\\\theta_1\\ \theta_2
        \end{bmatrix}$
        \item and $x_{i}' = \begin{bmatrix}
        1\\Temperature_i\\\#Occupants_i
        \end{bmatrix} = \begin{bmatrix}
        1 \\ x_i
        \end{bmatrix}$
        \item Notice the transpose in the equation! This is because $x_{i}$ is a column vector
    \end{itemize}

\end{frame}





\begin{frame}{We can expect the following}
    \begin{itemize}
        \item<+-> Demand increases, if \# occupants increases, then $\theta_{2}$ is likely to be positive
        
        \item<+-> Demand increases, if temperature increases, then $\theta_{1}$ is likely to be positive
        
        \item<+-> Base demand is independent of the temperature and the \# occupants, but, likely positive, thus $\theta_0$ is likely positive.
        
    \end{itemize}
\end{frame}


\section{Normal Equation}
\begin{frame}{Generalized Linear Regression Format}
\begin{itemize}[<+->]
	\item Assuming $N$ samples for training
	\item \# Features = $M$
\end{itemize}


   \pause \[\begin{bmatrix}
        \hat{y_{1}}\\
        \hat{y_{2}} \\
        \vdots \\
        \hat{y_{N}}
    \end{bmatrix}_{N \times 1}
    =     \begin{bmatrix}
        1 & x_{1,1} & x_{1,2} & \dots & x_{1,M}\\
        1 & x_{2,1} & x_{2,2} & \dots & x_{2,M}\\
        \vdots & \vdots & \vdots & \dots & \vdots\\
        1 & x_{N,1} & x_{N,2} & \dots & x_{N,M}\\
        % y_{2} \\
        % \vdots \\
        % y_{N}
    \end{bmatrix}_{N \times (M+1)}
    \begin{bmatrix}
        \theta_{0}\\
        \theta_{1}\\
        \vdots \\
        \theta_{M}\\
    \end{bmatrix}_{(M+1)\times 1}
   \]
   
%   \begin{center}
%       \textbf{Y}: N\times1; \textbf{X}: N\times(M+1); $\mathbf{\theta}$: (M+1)\times1 
%   \end{center}
   
  \pause  \begin{tcolorbox}
   \begin{center}
       
   

   $ \yhat = \mX\vtheta$
   \end{center}
   \end{tcolorbox}

   
\end{frame}


\begin{frame}{Relationships between feature and target variables}
    \begin{itemize}
    	\item  There could be different $\theta_{0}, \theta_{1} \dots \theta_{M}$. Each of them can represents a relationship.
    	\item  Given multiples values of $\theta_{0}, \theta_{1} \dots \theta_{M}$ how to choose which is the  best?
    	\item Let us consider an example in 2d
    \end{itemize}
   
    
   
    
\end{frame}

\begin{frame}{Relationships between feature and target variables}
Out of the three fits, which one do we choose? \vspace{10pt}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$x$,
ylabel=$y$,
xmin=-1,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside,
legend pos=outer north east
]

\addplot[mark=none,blue] {x};\addlegendentry{$\hat{y}=0+1x$}
\addplot[mark=none,yellow] {x+2};\addlegendentry{$\hat{y}=2+1x$}
\addplot[mark=none,green] {2*x-2};\addlegendentry{$\hat{y}=-2+2x$}
\addplot[only marks, mark=*]
coordinates{ % plot 1 dataset
	(0,0)
	(1,1)
	(2,2)
	(3,3)
	% more points...
}; \addlegendentry{Train data}


% plot 1 legend entry


\end{axis}

\end{tikzpicture}

\end{frame}


\begin{frame}{Relationships between feature and target variables}
We have $\hat{y}=2+1x$ as one relationship.\vspace{10pt}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$x$,
ylabel=$y$,
xmin=-1,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside,
legend pos=outer north east
]


\addplot[mark=none,yellow] {x+2};\addlegendentry{$\hat{y}=2+1x$}
\addplot[only marks, mark=*]
coordinates{ % plot 1 dataset
	(0,0)
	(1,1)
	(2,2)
	(3,3)
	% more points...
}; 
\addplot[only marks, mark=*, yellow]
coordinates{ % plot 1 dataset
	(0,2)
	(1,3)
	(2,4)
	(3,5)
	% more points...
}; ;\addlegendentry{Train Data}

% plot 1 legend entry


\end{axis}

\end{tikzpicture}

\end{frame}


\begin{frame}{Relationships between feature and target variables}
How far is our estimated $\hat{y}$ from ground truth $y$?\vspace{10pt}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$x$,
ylabel=$y$,
xmin=-1,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside,
legend pos=outer north east
]


\addplot[mark=none,yellow] {x+2};\addlegendentry{$\hat{y}=2+1x$}
\addplot[only marks, mark=*]
coordinates{ % plot 1 dataset
	(0,0)
	(1,1)
	(2,2)
	(3,3)
	% more points...
}; 
\addplot[only marks, mark=*, yellow]
coordinates{ % plot 1 dataset
	(0,2)
	(1,3)
	(2,4)
	(3,5)
	% more points...
}; 
\draw (axis cs:0,0) -- node[left, ]{$\epsilon_{1}$}(axis cs:0,2);
\draw (axis cs:1,1) -- node[left,]{$\epsilon_{2}$}(axis cs:1,3);
\draw (axis cs:2,2) -- node[left,]{$\epsilon_{3}$}(axis cs:2,4);
\draw (axis cs:3,3) -- node[left,]{$\epsilon_{4}$}(axis cs:3,5);

% plot 1 legend entry


\end{axis}

\end{tikzpicture}

\end{frame}


\begin{frame}{Error terms}


	\begin{itemize}[<+->]
		\item $y_{i} = \yhati + \epsilon_{i}$ where $\epsilon_{i} \sim \mathcal{N}(0, \sigma^2)$
		\item \textbf{Critical Assumption}: $\epsilon_{i}$ are \textcolor{red}{independent and identically distributed (i.i.d.)}
		\item $y_{i}$ denotes the ground truth for $i^{th}$ sample
		\item $\yhati$ denotes the prediction for $i^{th}$ sample, where $\yhati = \vx_{i}\tp \vtheta$
		\item $\epsilon_{i}$ denotes the error/residual for $i^{th}$ sample
		\item $\theta_{0}, \theta_{1}$: The parameters of the linear regression
		\item   $  \epsilon_{i} = y_{i} - \yhati$
		\item     $\epsilon_{i} = y_{i} - (\theta_{0} + x_{i} \cdot \theta_{1})$

\end{itemize}





\end{frame}



\begin{frame}{Good fit}

\begin{itemize}
    \item<+-> $|\epsilon_{1}|$, $|\epsilon_{2}|$, $|\epsilon_{3}|$, ... should be small.
    \item<+-> 
${\text{minimize }} \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2$ - $L_{2}$ Norm
    \item<+-> 
${\text{minimize }} |\epsilon_{1}| + |\epsilon_{2}| + \dots + |\epsilon_{n}|$ - $L_{1}$ Norm
\end{itemize}
\end{frame}





\begin{frame}{Normal Equation}
\begin{itemize}[<+->]
    \item Model specification:
        \begin{center}
        \begin{tcolorbox}
            $ \vy = \mX\vtheta + \vepsilon$
        \end{tcolorbox}
        \end{center}
    \item To Learn: $\vtheta$
    \item Objective: ${\text{minimize }} \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2$
\end{itemize}
\end{frame}

\begin{frame}{Normal Equation}
    
\begin{equation*}
 \vepsilon = 
\begin{bmatrix}
    \epsilon_{1} \\
    \epsilon_{2} \\
    \vdots \\
    \epsilon_{N} \\
\end{bmatrix}
\end{equation*}
\\
\begin{center}
 \pause Objective:   Minimize $\vepsilon\tp\vepsilon$    
\end{center}
\end{frame}

\begin{frame}{Derivation of Normal Equation}
$$\vepsilon = \vy - \mX\vtheta$$

$$\vepsilon\tp\vepsilon = (\vy - \mX\vtheta)\tp(\vy - \mX\vtheta)$$

$$= \vy\tp\vy - 2\vy\tp\mX\vtheta + \vtheta\tp\mX\tp\mX\vtheta$$

This is what we wish to minimize
\end{frame}

\begin{frame}{Minimizing the objective function}
    
    
    $$\frac{\partial \vepsilon\tp \vepsilon}{\partial \vtheta} = \vzero$$
    
    
    
    \begin{itemize}
        \item $\frac{\partial }{\partial \vtheta} \vy\tp\vy = \vzero$
        \item $\frac{\partial }{\partial \vtheta}(-2\vy\tp\mX\vtheta ) = -2\mX\tp\vy$
        \item $\frac{\partial}{\partial\vtheta} (\vtheta\tp\mX\tp\mX\vtheta) = 2\mX\tp\mX\vtheta$
    \end{itemize}
    
    Substitute the values in the top equation
    
\end{frame}

\begin{frame}{Normal Equation derivation}
$$
    \vzero = -2\mX\tp\vy + 2\mX\tp\mX\vtheta
$$

$$
    \mX\tp\vy  = \mX\tp\mX\vtheta
$$

\begin{tcolorbox}
\begin{center}
        $\hat{\vtheta}_{OLS} = (\mX\tp\mX)\inv\mX\tp\vy$
\end{center}
\end{tcolorbox}

\end{frame}

\begin{frame}{Worked out example}
    \begin{center}
 \begin{tabular}{||c c||} 
 \hline
 x  & y \\ [0.5ex] 
 \hline\hline
 0 & 0 \\
 1 & 1 \\
 2 & 2 \\
 3 & 3 \\
 \hline
\end{tabular}
\end{center}

Given the data above, find $\theta_{0}$ and $\theta_{1}$.

\end{frame}


\begin{frame}{Scatter Plot}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
	xlabel=$x$,
	ylabel=$y$,
	axis x line*=bottom,
	axis y line*=left,
	xtick align=outside,
	ytick align=outside
	]
	\addplot[only marks, mark=*]
	coordinates{ % plot 1 dataset
		(0,0)
		(1,1)
		(2,2)
		(3,3)
		% more points...
	}; 
	
	% plot 1 legend entry
	\addlegendimage{/pgfplots/refstyle=plot_one}

\end{axis}
\end{tikzpicture}
    
\end{frame}


\begin{frame}{Worked out example}
$$\mX = \begin{bmatrix}
            1 & 0\\
            1 & 1\\
            1 & 2\\
            1 & 3
        \end{bmatrix}$$
        
$$\mX\tp = \begin{bmatrix}
            1&1&1&1\\
            0&1&2&3
        \end{bmatrix}$$
        
$$\mX\tp\mX = \begin{bmatrix}
            4 &6\\6&14
        \end{bmatrix}$$

Given the data above, find $\theta_{0}$ and $\theta_{1}$.
\end{frame}


\begin{frame}{Worked out example}
    $$(\mX\tp\mX)\inv = \frac{1}{20} \begin{bmatrix}
                14 & -6\\
                -6& 4
            \end{bmatrix}$$
            
    $$\mX\tp\vy = \begin{bmatrix}
            1&1&1&1\\
            0&1&2&3
            \end{bmatrix}\begin{bmatrix}
            0\\1\\2\\3
            \end{bmatrix}=\begin{bmatrix}
                6\\
                14
            \end{bmatrix}$$
\end{frame}


\begin{frame}{Worked out example}
    $$\vtheta = (\mX\tp\mX)\inv(\mX\tp\vy)$$
    
    $$\begin{bmatrix}
        \theta_{0}\\
        \theta_{1}
    \end{bmatrix} = \frac{1}{20} \begin{bmatrix}
    14 & -6\\
    -6& 4
    \end{bmatrix}\begin{bmatrix}
    6\\
    14
    \end{bmatrix} =
    \begin{bmatrix}
        0\\
        1
    \end{bmatrix}$$
\end{frame}

\begin{frame}{Scatter Plot}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$x$,
ylabel=$y$,
xmin=0,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside,
legend pos=outer north east
]
\addplot[mark=none, gray] {x};\addlegendentry{Fit ($\hat{y}=x)$}
\addplot[only marks, mark=*]
coordinates{ % plot 1 dataset
	(0,0)
	(1,1)
	(2,2)
	(3,3)
	% more points...
}; 


% plot 1 legend entry


\end{axis}

\end{tikzpicture}


\end{frame}



\begin{frame}{Effect of outlier}

    \begin{center}
 \begin{tabular}{||c c||} 
 \hline
 x  & y \\ [0.5ex] 
 \hline\hline
 1 & 1 \\
 2 & 2 \\
 3 & 3 \\
 4 & 0 \\
 \hline
\end{tabular}
\end{center}

Compute the $\theta_{0}$ and $\theta_{1}$.
\end{frame}

\begin{frame}{Scatter Plot}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$x$,
ylabel=$y$,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside,
legend pos=outer north east
]
\addplot[only marks, mark=*]
coordinates{ % plot 1 dataset
	(1,1)
	(2,2)
	(3,3)
	(4,0)
	% more points...
}; 
\node[label={180:{Outlier}},circle,fill,inner sep=2pt] at (axis cs:4,0) {};

% plot 1 legend entry
\addlegendimage{/pgfplots/refstyle=plot_one}

\end{axis}
\end{tikzpicture}

\end{frame}

\begin{frame}{Worked out example}
$$\mX = \begin{bmatrix}
1 & 1\\
1 & 2\\
1 & 3 \\
1 & 4
\end{bmatrix}$$

$$\mX\tp = \begin{bmatrix}
1&1&1&1\\
1&2&3&4
\end{bmatrix}$$

$$\mX\tp\mX = \begin{bmatrix}
4 &10\\10&30
\end{bmatrix}$$

Given the data above, find $\theta_{0}$ and $\theta_{1}$.
\end{frame}


\begin{frame}{Worked out example}
$$(\mX\tp\mX)\inv = \frac{1}{20} \begin{bmatrix}
30 & -10\\
-10& 4
\end{bmatrix}$$

$$\mX\tp\vy = \begin{bmatrix}
6\\
14
\end{bmatrix}$$
\end{frame}


\begin{frame}{Worked out example}
$$\vtheta = (\mX\tp\mX)\inv(\mX\tp\vy)$$

$$\begin{bmatrix}
\theta_{0}\\
\theta_{1}
\end{bmatrix} = 
\begin{bmatrix}
2\\
(-1/5)
\end{bmatrix}$$
\end{frame}

\begin{frame}{Scatter Plot}
\begin{tikzpicture}

\pgfplotsset{
	scale only axis,
}

\begin{axis}[
xlabel=$x$,
ylabel=$y$,
xmin=0.5,
axis x line*=bottom,
axis y line*=left,
xtick align=outside,
ytick align=outside,
legend pos=outer north east
]
\addplot[mark=none, gray] {2-x/5};\addlegendentry{Fit ($\hat{y}=2-x/5$)}
\addplot[only marks, mark=*]
coordinates{ % plot 1 dataset
	(1,1)
	(2,2)
	(3,3)
	(4,0)
	% more points...
}; 
\node[label={180:{Outlier}},circle,fill,inner sep=2pt] at (axis cs:4,0) {};

% plot 1 legend entry


\end{axis}

\end{tikzpicture}


\end{frame}



\section{Basis Expansion}

\begin{frame}{Variable Transformation}
    Transform the data, by including the higher power terms in the feature space. 
    
       
    \begin{center}
 \begin{tabular}{||c c||} 
 \hline
 t  & s \\ [0.5ex] 
 \hline\hline
 0 & 0 \\
 1 & 6 \\
 3 & 24 \\
 4 & 36 \\
 \hline
\end{tabular}
\end{center}

The above table represents the data before transformation
\end{frame}


\begin{frame}{Variable Transformation}
Add the higher degree features to the previous table
    
       
    \begin{center}
 \begin{tabular}{||c c c||} 
 \hline
 t  & $t^{2}$ & s \\ [0.5ex] 
 \hline\hline
 0 & 0&0 \\
 1 & 1&6 \\
 3 & 9&24 \\
 4 & 16&36 \\
 \hline
\end{tabular}
\end{center}

\begin{itemize}[<+->]
    \item The above table represents the data after transformation
    \item Now, we can write $\hat{s}=f(t, t^2)$
    \item Other transformations: $\log(x), x_1\times x_2$
\end{itemize}
\end{frame}

\begin{frame}{A big caveat: Linear in what?!\footnotemark}
\begin{enumerate}[<+->]
	\item $\hat{s}=\theta_0 + \theta_1*t$
	 is linear
	 \item Is $\hat{s}=\theta_0 + \theta_1*t + \theta_2*t^2$
	 linear?
	 \item Is $\hat{s}=\theta_0 + \theta_1*t + \theta_2*t^2 + \theta_3*\cos(t^3)$
	 linear?
	\item Is $\hat{s}=\theta_0 + \theta_1*t + {\rm e}^{\theta_2}*t$
	linear?
	\item All except \#4 are linear models! 
	\item Linear refers to the relationship between the parameters that you are estimating ($\vtheta$) and the outcome 
\end{enumerate}
\footnotetext[1]{\url{https://stats.stackexchange.com/questions/8689/what-does-linear-stand-for-in-linear-regression}}
\end{frame}

\begin{frame}{Basis Functions}
    \begin{itemize}
        \item Linear regression only refers to linear in the parameters
        \item We can perform an arbitrary nonlinear transformation $\phi(x)$ of the inputs $x$ and then linearly combine the components of this transformation.
        \item $\phi: \Real^D \rightarrow \Real^K$ is called the basis function
    \end{itemize} 
    
\end{frame}

\begin{frame}{Basis Functions}
    Some examples of basis functions:
    \begin{itemize}
        \item Polynomial basis: $\phi(x) = \{1, x, x^2, x^3, \dots\}$
        \item Fourier basis: $\phi(x) = \{1, \sin(x), \cos(x), \sin(2x), \cos(2x), \dots\}$
        \item Gaussian basis: $\phi(x) = \{1, \exp(-\frac{(x-\mu_1)^2}{2\sigma^2}), \exp(-\frac{(x-\mu_2)^2}{2\sigma^2}), \dots\}$
        \item Sigmoid basis: $\phi(x) = \{1, \sigma(x-\mu_1), \sigma(x-\mu_2), \dots\}$ where $\sigma(x) = \frac{1}{1+e^{-x}}$
    \end{itemize}
    
    \begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/basis.html}
    Interactive examples and visualizations of different basis functions
    \end{notebookbox}
    
\end{frame}

\section{Geometric Interpretation}
\begin{frame}{Linear Combination of Vectors}
\begin{itemize}[<+->]
    \item Let $\vv_{1},\vv_{2},\vv_{3},\dots,\vv_{i}$ be vectors in  $\Real^{D}$, where $D$ denotes the dimensions
    \item A linear combination of $\vv_{1},\vv_{2},\vv_{3},\dots,\vv_{i}$ is of the following form:
        \begin{equation*}
            \alpha_{1}\vv_{1}+			\alpha_{2}\vv_{2}+			\alpha_{3}\vv_{3}+
            \dots+\alpha_{i}\vv_{i}
        \end{equation*}
\end{itemize}
    
    where $\alpha_{1},\alpha_{2},\alpha_{3},\dots,\alpha_{i} \in \Real$
    
\end{frame}

\begin{frame}{Span of vectors}
\begin{itemize}[<+->]
    \item Let $v_{1},v_{2},\dots,v_{i}$ be vectors in  ${\rm I\!R}^{D}$, with $D$ dimensions
    \item The span of  $v_{1},v_{2},\dots,v_{i}$ is denoted by SPAN\{$v_{1},v_{2},\dots,v_{i} $\}:
        \begin{equation*}
        \{	\alpha_{1}v_{1}+			\alpha_{2}v_{2}+
        \dots+\alpha_{i}v_{i} \hspace{1em}\vert \hspace{1em}  \alpha_{1},\alpha_{2},\dots,\alpha_{i} \in {\rm I\!R}\}
        \end{equation*}
    \item It is the set of all vectors that can be generated by linear combinations of $v_{1},v_{2},\dots,v_{i}$
    \item If we stack the vectors $v_{1},v_{2},\dots,v_{i}$ as columns of a matrix $V$, then the span of $v_{1},v_{2},\dots,v_{i}$ is given as $V\alpha$ where $\alpha \in {\rm I\!R}^{i}$
\end{itemize}
\end{frame}

\begin{frame}{Example}
Find the span of ($\begin{bmatrix}
1 \\3
\end{bmatrix}, \begin{bmatrix}
2 \\1
\end{bmatrix}) $

\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/geometric-linear-regression.html}
Interactive geometric visualization of vector spans and linear regression
\end{notebookbox}

\includegraphics{../assets/linear-regression/figures/geoemetric-span-1.pdf}



\end{frame}

\begin{frame}{Example}

    \includegraphics{../assets/linear-regression/figures/geoemetric-span-2.pdf}

    We have $v_3 = v_1 + v_2$ \\
    We have $v_4 = v_1 - v_2$ \\


\end{frame}

\begin{frame}{Example}
    Simulating the above example in python using different values of $\alpha_1$ and $\alpha_2$


    \includegraphics{../assets/linear-regression/figures/geoemetric-span-3.pdf}
    
    Span(($v_1, v_2$)) $\in \mathcal{R}^2$
\end{frame}


\begin{frame}{Example}
Find the span of ($\begin{bmatrix}
1 \\2
\end{bmatrix}, \begin{bmatrix}
2 \\4
\end{bmatrix}) $

\begin{itemize}[<+->]
    \item Can we obtain a point (x, y) s.t. x = 3y?
    \item No
    \item Span of the above set is along the line y = 2x
\end{itemize}

\includegraphics[scale=0.6]{../assets/linear-regression/figures/geoemetric-span-4.pdf}


\end{frame}

\begin{frame}{Example}
Find the span of ($\begin{bmatrix}
1 \\1\\1
\end{bmatrix}, \begin{bmatrix}
2 \\-2\\2
\end{bmatrix}) $
\begin{itemize}[<+->]
    \item Visualization:
        \begin{center}
        \includegraphics[width=0.6\textwidth]{../assets/linear-regression/figures/geometric-1.pdf}
        \end{center}
    \item The span is the plane $z=x$ or $x_3=x_1$
\end{itemize}
\end{frame}

\begin{frame}{Geometric Interpretation}
Consider $\mX$ and $\vy$ as follows. 
$$
\mX=\left(\begin{array}{cc}
{1} & {2} \\
{1} & {-2} \\
{1} & {2}
\end{array}\right), \quad \vy=\left(\begin{array}{c}
{8.8957} \\
{0.6130} \\
{1.7761}
\end{array}\right)
$$
\begin{itemize}[<+->]
\item We are trying to learn $\vtheta$ for $\yhat=\mX\vtheta$ such that $\vert \vert \vy - \yhat \vert \vert_{2}$ is minimised
\item Consider the two columns of $\mX$. Can we write $\mX\vtheta$ as the span of ($\begin{bmatrix}
1 \\1\\1
\end{bmatrix}, \begin{bmatrix}
2 \\-2\\2
\end{bmatrix}$)?
\item We wish to find $\yhat$ such that 
$$
\underset{\yhat \in SPAN\{\bar{x_{1}},\bar{x_{2}},\dots,\bar{x_{D}}\} } \argmin \vert \vert \vy - \yhat \vert \vert_{2}
$$
\end{itemize}

\end{frame}


\begin{frame}{Geometric Interpretation}
Span of ($\begin{bmatrix}
1 \\1\\1
\end{bmatrix}, \begin{bmatrix}
2 \\-2\\2
\end{bmatrix}) $


\includegraphics[width=0.6\textwidth]{../assets/linear-regression/figures/geometric-2.pdf}

\end{frame}


\begin{frame}{Geometric Interpretation}	

    \includegraphics[width=0.6\textwidth]{../assets/linear-regression/figures/geometric-3.pdf}

    


\begin{itemize}[<+->]
\item We seek a $\yhat$ in the span of the columns of $\mX$ such that it is closest to $\vy$
\end{itemize}

\end{frame}


\begin{frame}{Geometric Interpretation}	

    \includegraphics[width=0.6\textwidth]{../assets/linear-regression/figures/geometric-4.pdf}

    


\begin{itemize}[<+->]
\item This happens when $\vy-\yhat\perp \vx_j \forall j$ or $\vx_j\tp(\vy-\yhat)=0$
\item $\mX\tp(\vy-\mX\vtheta)=0$
\item $\mX\tp\vy = \mX\tp\mX\vtheta$ or $\hat{\vtheta} =(\mX\tp\mX)\inv\mX\tp\vy$ 
\end{itemize}

\end{frame}




\section{Dummy Variables and Multicollinearity}
\begin{frame}{Multi-collinearity}
\begin{itemize}[<+->]
    \item There can be situations where inverse of $\mX\tp\mX$ is not computable
    \item This condition arises when the $|\mX\tp\mX|$ = 0 (determinant is zero)
    \item \textbf{Example:} Perfect multicollinearity
\end{itemize}
    
    \begin{equation}
    \mX = \begin{bmatrix}
    1 & 1& 2\\
    1 & 2& 4\\
    1 & 3& 6\\
    \end{bmatrix}
    \end{equation}
    
    \begin{itemize}[<+->]
        \item The matrix X is not full rank (rank = 2, not 3)
        \item Notice: Column 3 = 2 × Column 2 (perfect linear dependence)
        \item \textcolor{red}{Cannot uniquely solve for $\vtheta$!}
    \end{itemize} 
    \end{frame}
    
    
    
    
    
    \begin{frame}{Multi-collinearity: Definition and Problems}
    
    \begin{definitionbox}{Multicollinearity}
    Arises when predictor variables/features in $\mX$ can be expressed as a linear combination of others
    \end{definitionbox}
    
    \textbf{Types:}
    \begin{itemize}[<+->]
        \item \textbf{Perfect}: Exact linear relationship (determinant = 0)
        \item \textbf{High}: Strong but not perfect correlation (determinant $\approx 0$)
    \end{itemize}
    
    \pause
    \textbf{Problems caused:}
    \begin{itemize}[<+->]
        \item Unstable coefficient estimates (same data → different $\vtheta$)
        \item High variance: Small changes in data → Large changes in coefficients
        \item Can't interpret individual feature importance
    \end{itemize}
    \end{frame}
    
    \begin{frame}{Why Multicollinearity Causes Instability}
    
    \textbf{The core problem:} Multiple parameter combinations give identical results
    
    \begin{examplebox}{Simple Example}
    If $x_2 = 2x_1$ exactly, then:
    \\ $y = \theta_0 + \theta_1 x_1 + \theta_2 x_2$
    \\ $y = \theta_0 + \theta_1 x_1 + \theta_2 (2x_1)$
    \\ $y = \theta_0 + (\theta_1 + 2\theta_2) x_1$
    \end{examplebox}
    
    \pause
    \begin{itemize}[<+->]
        \item Many $(\theta_1, \theta_2)$ pairs give same prediction
        \item $(\theta_1=1, \theta_2=0)$ and $(\theta_1=3, \theta_2=-1)$ both work!
        \item Small noise ``chooses'' randomly between solutions
        \item Result: Wildly different coefficients for same data
    \end{itemize}
    \end{frame}
    
    \begin{frame}{Numerical Example: Why Coefficients Go Wild}
    \textbf{Dataset:} House prices with sq\_ft and sq\_m (perfectly correlated)
    
    \begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    Price (\$k) & sq\_ft & sq\_m \\
    \hline
    200 & 2000 & 186 \\
    300 & 3000 & 279 \\
    400 & 4000 & 372 \\
    \hline
    \end{tabular}
    \end{center}
    
    \pause
    \textbf{What happens with tiny noise?}
    \begin{itemize}[<+->]
        \item \textbf{Clean data:} $(\theta_1, \theta_2) = (0.1, 0)$
        \item \textbf{Add 0.1\% noise:} $(\theta_1, \theta_2) = (-2.5, 28.0)$
        \item Same predictions, completely different coefficients!
        \item Which feature is ``important''? Impossible to say!
    \end{itemize}
    
    \pause
    \footnotesize
    \begin{keypointsbox}{}
    \textbf{Solutions:} Drop one variable, or use regularization (Ridge/Lasso)
    \end{keypointsbox}
    \end{frame}
    
    \begin{frame}{Dummy Variables: The Problem}
    \textbf{Example:} Pollution in Delhi = P
    \begin{itemize}[<+->]
        \item Model specification:
            \begin{center}
            P = $\theta_{0}$ + $\theta_{1}$*\#Vehicles + $\theta_{2}$*
            \textit{Wind speed} + $\theta_{3}$ * \textit{Wind Direction}
            \end{center}
        \item But, wind direction is a categorical variable
        \item \textbf{Naive approach:} \{N:0, E:1, W:2, S:3 \}
        \item \textcolor{red}{Problem:} This incorrectly implies S$>$W$>$E$>$N (meaningless ordering!)
        \item Model assumes: S is ``3 times better'' than N for reducing pollution
    \end{itemize}
    \end{frame}
    
    \begin{frame}{One-Hot Encoding (N-1 Variables)}
    \textbf{Correct approach:} Use binary indicators for each category
    
    \begin{center}
    \textbf{N-1 encoding (recommended)}\\
    \vspace{1em}
    \begin{tabular}{|c|c|c|c|}
        \hline
        Wind Direction & Is North? & Is East? & Is West?\\
        \hline
        \hline
        North & 1 & 0 & 0 \\
        East & 0 & 1 & 0\\
        West & 0 & 0 & 1\\
        South & 0 & 0 & 0\\
        \hline
    \end{tabular}
    \end{center}
    
    \pause
    \begin{keypointsbox}{}
    South is the \textbf{reference category} - all others are compared to it
    \end{keypointsbox}
    \end{frame}
    
    
    \begin{frame}{Why Not N Variables?}
    \textbf{Full encoding (problematic):}
    
    \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    Wind & Is N? & Is E? & Is W? & Is S?\\
    \hline
    North & 1 & 0 & 0 & 0 \\
    East & 0 & 1 & 0 & 0\\
    West & 0 & 0 & 1 & 0\\
    South & 0 & 0 & 0 & 1\\
    \hline
    \end{tabular}
    \end{center}
    
    \pause
    \begin{alertbox}{Multicollinearity Problem!}
    Notice: Is\_N + Is\_E + Is\_W + Is\_S = 1 (always!)
    \\ One column is perfectly predictable from the others
    \end{alertbox}
    \end{frame}
    
    
    
    \begin{frame}{N-1 vs N Encoding: The Dummy Variable Trap}
    
    \textbf{Why N-1 encoding is better:}
    \begin{itemize}[<+->]
        \item \textbf{N encoding problem:} Perfect multicollinearity
        \item Mathematical relationship: Is\_S = 1 - (Is\_N + Is\_E + Is\_W)
        \item Matrix $\mX\tp\mX$ becomes non-invertible
        \item No unique solution exists!
    \end{itemize}
    
    \pause
    \begin{examplebox}{The Dummy Variable Trap}
    Always use N-1 dummy variables for N categories.
    \\ The omitted category becomes the \textbf{baseline/reference}.
    \end{examplebox}
    \end{frame}
    
    
    \begin{frame}{Binary Encoding}
    
    \begin{center}
    \begin{tabular}{|c|c|}
    \hline
    N & 00 \\
    E& 01\\
    W & 10\\
    S& 11\\
    \hline
    \end{tabular}\\
    \end{center}
    
    
    \vspace{1em}
    \begin{itemize}[<+->]
        \item W and S are related by one bit
        \item This introduces dependencies between them, and this can cause confusion in classifiers
    \end{itemize}
    \end{frame}
    
    \begin{frame}{Interpreting Dummy variables}
    \begin{center}
    \begin{tabular}{c|c}
    Gender& height\\
    \hline
    \hline
    F & \dots \\
    F & \dots \\
    F & \dots \\
    M & \dots \\
    M & \dots \\
    \end{tabular}
    
    \end{center}
    
    \pause Encoding
    
    \begin{center}
    \pause \begin{tabular}{c|c}
    Is Female& height\\
    \hline
    \hline
    1 & \dots \\
    1 & \dots \\
    1 & \dots \\
    0 & \dots \\
    0 & \dots \\
    \end{tabular}
    \end{center}
    
    \end{frame}
    
    \begin{frame}{Interpreting Dummy Variables}
    \begin{center}
        \pause \begin{tabular}{c|c}
            Is Female& height\\
            \hline
            \hline
            1 & 5 \\
            1 & 5.2 \\
            1 & 5.4 \\
            0 & 5.8 \\
            0 & 6 \\
        \end{tabular}
    \end{center}
    \begin{itemize}[<+->]
        \item Model: $height_{i}$ = $\theta_{0}$ + $\theta_{1}$ *  (Is Female) + $\epsilon_{i}$
        \item We get $\theta_0$ = 5.9 and $\theta_1$ = -0.7
        \item \textbf{$\theta_{0}$ = 5.9:} Average height of males (reference category)
        \item \textbf{$\theta_{1}$ = -0.7:} Difference between female and male heights
        \item \textbf{Female height} = $\theta_{0} + \theta_{1}$ = 5.9 + (-0.7) = 5.2
        \item \textbf{Male height} = $\theta_{0}$ = 5.9
        \item So $\theta_{1}$ = Avg(female) - Avg(male) = 5.2 - 5.9 = -0.7
    \end{itemize}
    \end{frame}
    
    \begin{frame}{Alternative Encoding: +1/-1 Scheme}
    \textbf{Instead of 0/1, we could use +1/-1:}
    
    \begin{examplebox}{+1/-1 Encoding}
    \(x_{i}=\left\{\begin{array}{ll}{+1} & {\text { if female }} \\ {-1} & {\text { if male }}\end{array}\right.\)
    \end{examplebox}
    
    \pause
    \begin{itemize}[<+->]
        \item Model: \(y_{i}=\theta_{0}+\theta_{1} x_{i}+\epsilon_{i}\)
        \item For females: \(y_{i}=\theta_{0}+\theta_{1} \cdot (+1) = \theta_{0}+\theta_{1}\)
        \item For males: \(y_{i}=\theta_{0}+\theta_{1} \cdot (-1) = \theta_{0}-\theta_{1}\)
    \end{itemize}
    
    \pause
    \footnotesize
    \begin{keypointsbox}{Interpretation}
    \begin{itemize}
        \item $\theta_{0}$ = overall average height across all people
        \item $\theta_{1}$ = half the difference between female and male heights
    \end{itemize}
    \end{keypointsbox}
\end{frame}

\begin{frame}{Summary: Categorical Variable Encodings}
\begin{center}
\begin{tabular}{|l|c|c|l|}
\hline
\textbf{Method} & \textbf{Good?} & \textbf{Variables} & \textbf{Issue} \\
\hline
\hline
Ordinal (0,1,2,3) & No & 1 & Implies fake ordering \\
\hline
Full One-Hot & No & N & Multicollinearity \\
\hline
N-1 One-Hot & Yes & N-1 & Recommended \\
\hline
Binary Encoding & Maybe & log₂(N) & Artificial relationships \\
\hline
+1/-1 Encoding & Yes & 1* & Only for 2 categories \\
\hline
\end{tabular}
\end{center}

\pause
\begin{definitionbox}{Best Practice}
Use \textbf{N-1 one-hot encoding} for categorical variables.
\\ Choose the most common category as reference.
\end{definitionbox}
\end{frame}

\section{Practice and Review}

\begin{frame}{Pop Quiz: Linear Regression}
\begin{enumerate}[<+->]
\item What is the geometric interpretation of least squares?
\item When does the normal equation have a unique solution?
\item How do polynomial features help with non-linear relationships?
\item What are the assumptions behind linear regression?
\end{enumerate}
\end{frame}

\begin{frame}{Critical Assumptions of Linear Regression}
\textbf{Before using linear regression, verify these assumptions:}
\begin{itemize}[<+->]
\item \textbf{Linearity}: Relationship between $\vx$ and $y$ is linear
\item \textbf{Independence}: Observations are independent of each other
\item \textbf{Homoscedasticity}: Error variance is constant across all values of $\vx$
\item \textbf{Normality}: Errors are normally distributed (for inference)
\item \textbf{No Multicollinearity}: Features are not highly correlated
\end{itemize}

\pause
\textbf{Violation Consequences:}
\begin{itemize}[<+->]
\item Biased coefficient estimates
\item Invalid confidence intervals
\item Poor prediction performance
\end{itemize}
\end{frame}

\begin{frame}{Key Takeaways}
\begin{itemize}[<+->]
\item \textbf{Linear Model}: Assumes linear relationship between features and target
\item \textbf{Least Squares}: Minimizes sum of squared residuals
\item \textbf{Normal Equation}: Closed-form solution when $\mX\tp\mX$ is invertible
\item \textbf{Geometric View}: Projection onto column space of design matrix
\item \textbf{Feature Engineering}: Basis expansion enables non-linear modeling
\item \textbf{Foundation}: Building block for more complex models
\end{itemize}
\end{frame}

\end{document}
