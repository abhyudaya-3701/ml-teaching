\documentclass[usenames,dvipsnames]{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/styles/conventions}

\title{Cross-Validation}
\date{\today}
\author{Nipun Batra and teaching staff}
\institute{IIT Gandhinagar}

\begin{document}
	\maketitle

\setcounter{popquiz}{0}
\section{Introduction to Cross-Validation}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

\begin{frame}{Our General Training Flow}
\includegraphics[width = 0.9\textwidth]{../assets/cross-validation/diagrams/general-workflow}
\footnotesize
\cleanitemize{
	\item Does not use the full dataset for training and does not test on the full dataset
	\item No way to optimize hyperparameters
	\item This simple train/test split has limitations we need to address
}
\end{frame}

\popquiz{
\textbf{What are the main limitations of using only a single train/test split?}
}{
	\begin{itemize}[<*>]
		\item {\color{magenta}Does not utilize the full dataset for training}
		\item {\color{magenta}Cannot optimize hyperparameters systematically}
		\item {\color{magenta}Results depend on the particular split chosen}
		\item {\color{magenta}May not get reliable performance estimates}
	\end{itemize}
}

\section{Full Dataset Utilization}

\begin{frame}{How to use the full dataset for training?}
\cleanitemize{
	\item Over multiple iterations, use different parts of the dataset for training and testing
	\item Typically done via different random splits of the dataset
	\item \textbf{Challenge:} How to ensure systematic evaluation?
	\item May not use every data point for training or testing with random splits
	\item May be computationally expensive
}
\end{frame}

\section{K-Fold Cross-Validation}

\begin{frame}{K-Fold Cross-Validation: Utilize Full Dataset for Testing}
\includegraphics[width = \textwidth]{../assets/cross-validation/diagrams/cross-validation-train-test}
\end{frame}

\begin{frame}{K-Fold Cross-Validation: Utilize Full Dataset for Testing}
\cleanitemize{
	\item Each data point is used for testing exactly once
	\item Each data point is used for training $(k-1)/k$ of the time
	\item Provides more robust performance estimates
}
\end{frame}

\popquiz{
\textbf{If you have 100 data points and use 5-fold cross-validation, how many data points are used for training in each fold?}
}{
{\color{magenta}\textbf{80 data points}} (4 out of 5 folds $= 4/5 \times 100 = 80$)
}

\section{Hyperparameter Optimization}

\begin{frame}{Optimizing Hyperparameters via the Validation Set}
\includegraphics[width = \textwidth]{../assets/cross-validation/diagrams/validation-workflow}
\end{frame}

\begin{frame}{Optimizing Hyperparameters via the Validation Set}
\cleanitemize{
	\item Validation set helps select the best hyperparameters
	\item Test set remains untouched until final evaluation
	\item This prevents overfitting to the test set
}
\end{frame}

\section{Nested Cross-Validation}

\begin{frame}{Nested Cross-Validation Process}
Divide your training set into $k$ equal parts.\\
Cyclically use 1 part as ``validation set'' and the rest for training.\\
Here $k = 4$
\begin{center}
\includegraphics[scale=0.7]{../assets/cross-validation/diagrams/cross-validation.pdf}
\end{center}
\cleanitemize{
	\item Each fold provides one validation score
	\item Process is systematic and exhaustive
}
\end{frame}

\popquiz{
\textbf{What is the difference between simple cross-validation and nested cross-validation?}
}{
	\begin{itemize}[<*>]
		\item {\color{magenta}\textbf{Simple CV}: Used for model evaluation only}
		\item {\color{magenta}\textbf{Nested CV}: Outer loop for model evaluation, inner loop for hyperparameter tuning}
		\item {\color{magenta}\textbf{Nested CV} provides unbiased estimates when doing hyperparameter search}
	\end{itemize}
}

\begin{frame}{Cross-Validation Results}
Average out the validation accuracy across all the folds\\
Use the hyperparameters with highest average validation accuracy\\
\includegraphics[width = \textwidth]{../assets/cross-validation/diagrams/cross-validation-avg.pdf}
\cleanitemize{
	\item Final model is trained on entire training set
	\item Standard deviation gives confidence in results
}
\end{frame}

\popquiz{
\textbf{Why do we average the results across all folds instead of picking the best single fold?}
}{
	\begin{itemize}[<*>]
		\item {\color{magenta}Single fold results can be misleading due to data variance}
		\item {\color{magenta}Averaging provides more robust performance estimates}
		\item {\color{magenta}Reduces impact of lucky/unlucky splits}
		\item {\color{magenta}Standard deviation indicates reliability of the estimate}
	\end{itemize}
}

\section{Cross-Validation Variants}

\begin{frame}{Leave-One-Out Cross-Validation (LOOCV)}
\cleanitemize{
	\item Special case where $k = n$ (number of data points)
	\item Each fold uses exactly one data point for testing
	\item \textbf{Advantages:}
	\cleanitemize{
		\item Maximum use of data for training
		\item Deterministic (no randomness)
	}
	\item \textbf{Disadvantages:}
	\cleanitemize{
		\item Computationally expensive
		\item High variance in estimates
	}
}
\end{frame}

\begin{frame}{Stratified Cross-Validation}
\cleanitemize{
	\item Maintains class distribution in each fold
	\item Important for imbalanced datasets
	\item Each fold has approximately same proportion of classes
	\item \textbf{Example:} If dataset is 70\% class A, 30\% class B, each fold maintains this ratio
	\item Reduces variance in performance estimates
}
\end{frame}

\popquiz{
\textbf{You have a binary classification dataset with 90\% negative and 10\% positive examples. Why is stratified cross-validation important here?}
}{
	\begin{itemize}[<*>]
		\item {\color{magenta}Regular CV might create folds with very few (or zero) positive examples}
		\item {\color{magenta}This would give misleading performance estimates}
		\item {\color{magenta}Stratified CV ensures each fold has $\sim$10\% positive examples}
		\item {\color{magenta}Results in more reliable and consistent evaluation}
	\end{itemize}
}

\section{Time Series Cross-Validation}

\begin{frame}{Time Series Cross-Validation}
\cleanitemize{
	\item Regular CV assumes data points are independent
	\item Time series data has temporal dependencies
	\item \textbf{Forward Chaining:} Train on past, test on future
	\item \textbf{Rolling Window:} Fixed-size training window
	\item \textbf{Expanding Window:} Growing training set over time
	\item Never use future data to predict past!
}
\end{frame}

\section{Common Pitfalls and Best Practices}

\begin{frame}{Common Cross-Validation Mistakes}
\cleanitemize{
	\item \textbf{Data Leakage:} Information from test set influences training
	\item \textbf{Incorrect Splitting:} Not accounting for grouped data
	\item \textbf{Overfitting to CV:} Too much hyperparameter tuning
	\item \textbf{Wrong Preprocessing:} Scaling on entire dataset before splitting
	\item \textbf{Ignoring Class Imbalance:} Not using stratified CV when needed
}
\end{frame}

\popquiz{
\textbf{What's wrong with computing mean and standard deviation on the entire dataset before doing cross-validation?}
}{
	\begin{itemize}[<*>]
		\item {\color{magenta}This causes data leakage!}
		\item {\color{magenta}Test fold statistics influence the training preprocessing}
		\item {\color{magenta}Should compute statistics only on training folds}
		\item {\color{magenta}Apply same transformation to corresponding test fold}
		\item {\color{magenta}This gives more realistic performance estimates}
	\end{itemize}
}

\section{Summary and Key Takeaways}

\begin{frame}{Cross-Validation: Key Benefits}
\cleanitemize{
	\item \textbf{Better Data Utilization:} Every point used for both training and testing
	\item \textbf{Robust Evaluation:} Multiple train/test splits reduce variance
	\item \textbf{Hyperparameter Tuning:} Systematic way to select best parameters
	\item \textbf{Model Comparison:} Fair comparison between different algorithms
	\item \textbf{Confidence Estimates:} Standard deviation indicates reliability
}
\end{frame}

\begin{frame}{When to Use Different CV Types}
\cleanitemize{
	\item \textbf{K-Fold (k=5,10):} General purpose, most common
	\item \textbf{Stratified:} Imbalanced classification problems
	\item \textbf{LOOCV:} Small datasets, when computational cost is acceptable
	\item \textbf{Time Series CV:} Temporal data with dependencies
	\item \textbf{Nested CV:} When doing extensive hyperparameter search
}
\end{frame}

\begin{frame}{Cross-Validation Best Practices}
\cleanitemize{
	\item Always preprocess within each fold separately
	\item Use stratification for classification problems
	\item Report mean $\pm$ standard deviation
	\item Don't overfit to cross-validation results
	\item Consider computational cost vs. benefit trade-off
	\item Use nested CV for unbiased hyperparameter search
}
\end{frame}

\begin{frame}{Next time: Ensemble Learning}
\cleanitemize{
\item How to combine various models?
\item Why combine multiple models?
\item How can we reduce bias?
\item How can we reduce variance?
\item Bootstrap aggregating (Bagging)
\item Boosting methods
}
\end{frame}

\end{document}
