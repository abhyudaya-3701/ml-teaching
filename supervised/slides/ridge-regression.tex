\documentclass{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/styles/conventions}

\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}

%\beamerdefaultoverlayspecification{<+->}

\title{Ridge Regression}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle

\begin{frame}{Outline}
\tableofcontents
\end{frame}

\section{Motivation: The Problem of Overfitting}

\begin{frame}{The Problem: Overfitting in Linear Regression}
\begin{alertbox}{Overfitting Challenge}
As model complexity increases (higher polynomial degree), we often observe:
\begin{itemize}
\item Training error decreases
\item Test error increases  
\item Model coefficients become very large
\end{itemize}
\end{alertbox}
\pause

\begin{keypointsbox}{Key Insight}
Large coefficient magnitudes often indicate overfitting!
\end{keypointsbox}
\pause

In polynomial $f(x) = c_0 + c_1x + c_2x^2 + \dots + c_dx^d$, watch $\max|c_i|$
\end{frame}
  
\begin{frame}{Demonstration: Polynomial Degree vs Overfitting}
\vspace{0.4cm}
\only<1>{\begin{figure}\includegraphics[width=0.8\linewidth]{../assets/ridge-regression/figures/lin_1.pdf}\caption{Base Data Set}
\end{figure}}
\only<2>{\begin{figure}\includegraphics[width=0.8\linewidth]{../assets/ridge-regression/figures/lin_plot_1.pdf}\caption{Fit with Degree 1 - Underfitting}
\end{figure}}
\only<3>{\begin{figure}\includegraphics[width=0.8\linewidth]{../assets/ridge-regression/figures/lin_plot_3.pdf}\caption{Fit with Degree 3 - Good Fit}
\end{figure}}
\only<4>{\begin{figure}\includegraphics[width=0.8\linewidth]{../assets/ridge-regression/figures/lin_plot_6.pdf}\caption{Fit with Degree 6 - Starting to Overfit}
\end{figure}}
\only<5>{\begin{figure}\includegraphics[width=0.8\linewidth]{../assets/ridge-regression/figures/lin_plot_11.pdf}\caption{Fit with Degree 11 - Severe Overfitting}
\end{figure}}
\end{frame}  

\begin{frame}{Coefficient Explosion with Overfitting}
\vspace{0.1cm}
\begin{keypointsbox}{Key Observation}
As polynomial degree increases $\rightarrow$ coefficients grow exponentially!
\end{keypointsbox}
\vspace{0.1cm}
\begin{figure}\includegraphics[width=0.7\linewidth]{../assets/ridge-regression/figures/lin_plot_coef.pdf}\caption{{\small Coefficient Magnitudes vs Polynomial Degree}}\end{figure}
\end{frame}

\begin{frame}{The Central Question}
\begin{alertbox}{Critical Question}
How can we control coefficient magnitudes to prevent overfitting?
\end{alertbox}
\pause

\begin{keypointsbox}{Answer Preview}
Ridge regression adds a penalty term to shrink coefficients!
\end{keypointsbox}
\end{frame}

\stepcounter{popquiz}
\begin{frame}{Pop Quiz \thepopquiz}
\begin{popquizbox}{Pop Quiz \thepopquiz}
Which statement about overfitting is TRUE?
\begin{enumerate}[A)]
\item Higher polynomial degree always improves generalization
\item Large coefficients indicate good model fit
\item Overfitting occurs when training error $>>$ test error
\item Overfitting occurs when training error $<<$ test error
\end{enumerate}
\end{popquizbox}
\end{frame}

\begin{frame}{Answer: Pop Quiz \thepopquiz}
\begin{popquizbox}{Answer: Pop Quiz \thepopquiz}
\textbf{D) Overfitting occurs when training error $<<$ test error}

\vspace{0.3cm}
Explanation:
\begin{itemize}
\item Training error becomes very small (model memorizes training data)
\item Test error remains large (model fails to generalize)  
\item Large gap indicates overfitting
\end{itemize}
\end{popquizbox}
\end{frame}

\section{Ridge Regression Formulation}

\begin{frame}{Solution: Regularization}
\begin{theorembox}{Ridge Regression Approach}
Add a penalty term to control coefficient magnitudes:
\end{theorembox}
\pause

\begin{definitionbox}{Constrained Formulation}
\begin{align*}
\min_{\vtheta} & \quad \left(\vy-\mX\vtheta\right)^T\left(\vy-\mX\vtheta\right) \\
\text{subject to} & \quad \vtheta^T\vtheta \leq S
\end{align*}
where $S > 0$ controls the size of the coefficient vector.
\end{definitionbox}
\end{frame}

\begin{frame}{Lagrangian Formulation}
\begin{theorembox}{Equivalence Theorem}
The constrained problem is equivalent to the unconstrained:
$$\min_{\vtheta} \quad \left(\vy-\mX\vtheta\right)^T\left(\vy-\mX\vtheta\right) + \lambda \vtheta^T\vtheta$$
where $\lambda \geq 0$ is the regularization parameter.
\end{theorembox}
\pause

\begin{keypointsbox}{Key Insight}
This transforms a constrained optimization into an unconstrained one with a penalty term.
\end{keypointsbox}
\end{frame}

\begin{frame}{Understanding the Ridge Penalty}
\begin{align}
J(\vtheta) &= \underbrace{\left(\vy-\mX\vtheta\right)^T\left(\vy-\mX\vtheta\right)}_{\text{Fit to data (MSE)}} + \underbrace{\lambda \vtheta^T\vtheta}_{\text{Penalty term}} \\[0.5em]
&= \text{MSE}(\vtheta) + \lambda \|\vtheta\|_2^2
\end{align}
\pause

\begin{keypointsbox}{Key Components}
\begin{itemize}
\item \textbf{Data fitting term}: Ensures good fit to training data
\item \textbf{Regularization term}: $L_2$ penalty shrinks coefficients toward zero
\item \textbf{$\lambda$}: Controls trade-off between fitting vs. regularization
\end{itemize}
\end{keypointsbox}
\end{frame}

\begin{frame}{Effect of Regularization Parameter $\lambda$}
\begin{keypointsbox}{Parameter Effects}
\begin{itemize}
\item $\lambda = 0$: No regularization (standard linear regression)
\item $\lambda$ small: Light regularization (slight shrinkage)
\item $\lambda$ large: Heavy regularization (strong shrinkage)
\item $\lambda \to \infty$: Extreme regularization (coefficients $\to 0$)
\end{itemize}
\end{keypointsbox}
\pause

\begin{alertbox}{Key Trade-off}
Higher $\lambda$ = more regularization = more bias, less variance
\end{alertbox}
\end{frame}

\begin{frame}{Geometric Interpretation}
\begin{figure}
\centering
\begin{tikzpicture}[scale=0.7]
\begin{axis}[
    width=8cm, height=5cm,
    xlabel={$\theta_0$ (intercept)}, ylabel={$\theta_1$ (slope)},
    xmin=-0.5, xmax=2.5, ymin=-0.5, ymax=0.8,
    grid=major, grid style={thin,gray!30},
    legend to name=legend3,
    axis equal=false
]
% Constraint circle for Ridge (centered at origin, radius sqrt(0.387) â‰ˆ 0.62)
\addplot[blue, thick, domain=0:360, samples=100] ({0.62*cos(x)}, {0.62*sin(x)});
% OLS solution point
\addplot[only marks, mark=*, mark size=4pt, red] coordinates {(2.0, -0.2)};
% Ridge solution point  
\addplot[only marks, mark=square*, mark size=4pt, green!70!black] coordinates {(0.565, 0.261)};
% MSE contour ellipses (simplified representation centered around OLS)
\addplot[orange, thick, domain=0:360, samples=100] ({0.3*cos(x) + 2.0}, {0.15*sin(x) - 0.2});
\addplot[orange, domain=0:360, samples=100] ({0.6*cos(x) + 2.0}, {0.3*sin(x) - 0.2});
\addplot[orange, domain=0:360, samples=100] ({0.9*cos(x) + 2.0}, {0.45*sin(x) - 0.2});
\legend{Constraint $\|\vtheta\|^2 \leq S$, OLS, Ridge, MSE contours}
\end{axis}
\end{tikzpicture}
\vspace{-0.2cm}
\ref{legend3}
\caption{{\small Ridge solution where MSE contours touch constraint region}}
\end{figure}

\vspace{-0.2cm}
\begin{keypointsbox}{Key Insight}
{\small Ridge finds the minimum MSE point within the constraint $\|\vtheta\|_2^2 \leq S$}
\end{keypointsbox}
\end{frame}

\section{Mathematical Derivation}

\begin{frame}{Mathematical Derivation: Step 1}
\begin{codebox}{Step 1: Set up the Lagrangian}
{\small For the constrained optimization problem:}
\begin{align*}
\min_{\vtheta} & \quad \left(\vy-\mX\vtheta\right)^T\left(\vy-\mX\vtheta\right) \\
\text{s.t.} & \quad \vtheta^T\vtheta \leq S
\end{align*}

{\small The Lagrangian is:}
$$L(\vtheta, \lambda) = \left(\vy-\mX\vtheta\right)^T\left(\vy-\mX\vtheta\right) + \lambda\left(\vtheta^T\vtheta - S\right)$$
{\small where $\lambda \geq 0$ is the Lagrange multiplier.}
\end{codebox}
\end{frame}

\begin{frame}{Mathematical Derivation: Step 2}
\begin{codebox}{Step 2: Apply KKT Conditions}
For optimality, we need:
\begin{align}
\frac{\partial L}{\partial \vtheta} &= 0 \quad \text{(stationarity)} \\
\lambda &\geq 0 \quad \text{(dual feasibility)} \\
\vtheta^T\vtheta - S &\leq 0 \quad \text{(primal feasibility)} \\
\lambda(\vtheta^T\vtheta - S) &= 0 \quad \text{(complementary slackness)}
\end{align}
\end{codebox}
\pause

\begin{keypointsbox}{Two Cases}
\begin{itemize}
\item \textbf{Case 1:} $\lambda = 0 \Rightarrow$ No constraint active (standard OLS)
\item \textbf{Case 2:} $\lambda > 0 \Rightarrow$ $\vtheta^T\vtheta = S$ (constraint is tight)
\end{itemize}
\end{keypointsbox}
\end{frame}

\begin{frame}{Mathematical Derivation: Step 3}
\begin{codebox}{Step 3: Compute the Gradient}
Taking the derivative of the Lagrangian with respect to $\vtheta$:
\begin{align}
\frac{\partial L}{\partial \vtheta} &= \frac{\partial}{\partial \vtheta}\left[\left(\vy-\mX\vtheta\right)^T\left(\vy-\mX\vtheta\right) + \lambda \vtheta^T\vtheta\right] \\[0.3em]
&= \frac{\partial}{\partial \vtheta}\left[\vy^T\vy - 2\vy^T\mX\vtheta + \vtheta^T\mX^T\mX\vtheta + \lambda \vtheta^T\vtheta\right] \\[0.3em]
&= -2\mX^T\vy + 2\mX^T\mX\vtheta + 2\lambda\vtheta
\end{align}
\end{codebox}
\end{frame}

\begin{frame}{Mathematical Derivation: Step 4}
\begin{codebox}{Step 4: Set Gradient to Zero}
Setting $\frac{\partial L}{\partial \vtheta} = 0$:
\begin{align}
-2\mX^T\vy + 2\mX^T\mX\vtheta + 2\lambda\vtheta &= 0 \\[0.3em]
-\mX^T\vy + (\mX^T\mX + \lambda\mI)\vtheta &= 0 \\[0.3em]
(\mX^T\mX + \lambda\mI)\vtheta &= \mX^T\vy
\end{align}
\end{codebox}
\pause

\begin{theorembox}{Ridge Regression Solution}
$$\hat{\vtheta}_{\text{ridge}} = (\mX^T\mX + \lambda\mI)^{-1}\mX^T\vy$$
\end{theorembox}

Compare with OLS: $\hat{\vtheta}_{\text{OLS}} = (\mX^T\mX)^{-1}\mX^T\vy$
\end{frame}

\begin{frame}{Effect of Regularization Parameter $\lambda$}
\vspace{0.4cm}
\only<1>{\begin{figure}\includegraphics[width=0.8\linewidth]{../assets/ridge-regression/figures/ridge_1.pdf}\caption{$\lambda = 1$ - Mild Regularization}
\end{figure}}
\only<2>{\begin{figure}\includegraphics[width=0.8\linewidth]{../assets/ridge-regression/figures/ridge_10.pdf}\caption{$\lambda = 10$ - Moderate Regularization}
\end{figure}}
\only<3>{\begin{figure}\includegraphics[width=0.8\linewidth]{../assets/ridge-regression/figures/ridge_1000.pdf}\caption{$\lambda = 1000$ - Heavy Regularization}
\end{figure}}
\end{frame}

%\begin{frame}{Effect of $\mu$ on higher degree fits}
%\vspace{0.4cm}
%\only<1>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/ridge_1_16.pdf}\caption{$\mu = 1$ when Degree $= 16$}
%\end{figure}}
%\only<2>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/ridge_100_16.pdf}\caption{$\mu = 100$  when Degree $= 16$}
%\end{figure}}
%\only<3>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/ridge_100000_16.pdf}\caption{$\mu = 100000$  when Degree $= 16$}
%\end{figure}}
%\end{frame}

\stepcounter{popquiz}
\begin{frame}{Pop Quiz \thepopquiz}
\begin{popquizbox}{Pop Quiz \thepopquiz}
What happens to the Ridge regression solution as $\lambda \to \infty$?
\begin{enumerate}[A)]
\item Coefficients approach the OLS solution
\item Coefficients approach zero
\item Solution becomes undefined
\item Training error becomes zero
\end{enumerate}
\end{popquizbox}
\end{frame}

\begin{frame}{Answer: Pop Quiz \thepopquiz}
\begin{popquizbox}{Answer: Pop Quiz \thepopquiz}
\textbf{B) Coefficients approach zero}

\vspace{0.3cm}
As $\lambda \to \infty$, the penalty term dominates:
$$\hat{\vtheta}_{\text{ridge}} = (\mX^T\mX + \lambda\mI)^{-1}\mX^T\vy \approx \lambda^{-1}\mI \mX^T\vy \to \mathbf{0}$$
\end{popquizbox}
\end{frame}

\begin{frame}{Coefficient Shrinkage: Visual Evidence}
\begin{figure}\includegraphics[width=0.8\linewidth]{../assets/ridge-regression/figures/rid_reg-without-lim.pdf}\caption{Coefficient Magnitudes vs $\lambda$ (Real Estate Dataset)}
\end{figure}

\begin{alertbox}{Important Question}
Do coefficients ever become exactly zero?
\end{alertbox}
\end{frame}

\begin{frame}{Ridge Coefficient Behavior}
\begin{figure}\includegraphics[width=0.8\linewidth]{../assets/ridge-regression/figures/rid_reg-with-lim.pdf}\caption{Ridge Coefficients Shrink but Never Reach Zero}
\end{figure}
\end{frame}

\begin{frame}{Ridge vs. Lasso: Key Difference}
\begin{keypointsbox}{Coefficient Behavior Comparison}
\begin{itemize}
\item \textbf{Ridge ($L_2$):} Coefficients shrink toward zero but remain non-zero
\item \textbf{Lasso ($L_1$):} Coefficients can become exactly zero (feature selection)
\end{itemize}
\end{keypointsbox}
\pause

\begin{alertbox}{Important Insight}
Ridge provides shrinkage, Lasso provides selection!
\end{alertbox}
\end{frame}

\begin{frame}{Ridge Regression Solution}
\begin{theorembox}{Ridge Solution Formula}
$$\hat{\vtheta}_{\text{ridge}} = (\mX^T\mX + \lambda\mI)^{-1}\mX^T\vy$$
\end{theorembox}
\end{frame}

\begin{frame}{Key Property 1: Always Invertible}
\begin{theorembox}{Invertibility Guarantee}
$(\mX^T\mX + \lambda\mI)$ is always positive definite for $\lambda > 0$
\end{theorembox}
\pause

\begin{keypointsbox}{Why This Matters}
\begin{itemize}
\item No singularity issues (unlike OLS)
\item Always has unique solution
\item Handles multi-collinearity gracefully
\end{itemize}
\end{keypointsbox}
\end{frame}

\begin{frame}{Key Property 2: Coefficient Shrinkage}
\begin{theorembox}{Shrinkage Effect}
Ridge regression shrinks coefficients toward zero (but not exactly zero)
\end{theorembox}
\pause

\begin{keypointsbox}{Shrinkage Benefits}
\begin{itemize}
\item Reduces overfitting
\item Stabilizes coefficient estimates
\item Improves generalization
\end{itemize}
\end{keypointsbox}
\end{frame}

\begin{frame}{Key Property 3: Bias-Variance Trade-off}
\begin{theorembox}{Trade-off Effect}
Ridge regression increases bias but reduces variance
\end{theorembox}
\pause

\begin{keypointsbox}{Net Effect}
\begin{itemize}
\item Total error often decreases
\item Better generalization to new data
\item Controlled by $\lambda$ parameter
\end{itemize}
\end{keypointsbox}
\end{frame}

\section{Hyperparameter Selection}

\begin{frame}{Choosing the Regularization Parameter $\lambda$}
\begin{alertbox}{Hyperparameter Selection}
How do we choose the optimal value of $\lambda$?
\end{alertbox}
\pause

\begin{theorembox}{Cross-Validation Approach}
\begin{enumerate}
\item Split data into training and validation sets (k-fold CV)
\item For each candidate $\lambda$ value:
   \begin{itemize}
   \item Train ridge model on training data
   \item Compute validation error
   \end{itemize}
\item Select $\lambda$ that minimizes validation error
\item Retrain on full dataset with chosen $\lambda$
\end{enumerate}
\end{theorembox}
\end{frame}

\begin{frame}{Cross-Validation for Ridge Regression}
\begin{figure}
\centering
\includegraphics[width=0.75\linewidth]{ridge-cv.pdf}
\caption{{\small Cross-validation curve showing optimal $\lambda$}}
\end{figure}

\vspace{-0.1cm}
\begin{keypointsbox}{CV Pattern}
{\small 
\begin{itemize}
\item Small $\lambda$: Overfitting  \quad Large $\lambda$: Underfitting
\item Optimal $\lambda$: Best trade-off
\end{itemize}
}
\end{keypointsbox}
\end{frame}

\begin{frame}{Bias-Variance Trade-off in Ridge Regression}
\begin{theorembox}{Bias-Variance Decomposition}
Total Error = Bias$^2$ + Variance + Irreducible Error
\end{theorembox}
\pause

\begin{keypointsbox}{Ridge Effect}
Regularization increases bias but reduces variance, often leading to lower total error.
\end{keypointsbox}
\end{frame}

\begin{frame}{Small vs Large Regularization}
\begin{columns}
\begin{column}{0.48\textwidth}
\begin{figure}
\includegraphics[width=\linewidth]{../assets/ridge-regression/figures/ridge_new_0_17.pdf}
\end{figure}
\textbf{Small $\lambda$ ($\lambda \to 0$):}
\begin{itemize}
\item Low bias
\item High variance
\item Risk of overfitting
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\begin{figure}
\includegraphics[width=\linewidth]{../assets/ridge-regression/figures/ridge_new_1_17.pdf}
\end{figure}
\textbf{Large $\lambda$ ($\lambda \to \infty$):}
\begin{itemize}
\item High bias
\item Low variance  
\item Risk of underfitting
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\stepcounter{popquiz}
\begin{frame}{Pop Quiz \thepopquiz}
\begin{popquizbox}{Pop Quiz \thepopquiz}
In ridge regression, as we increase $\lambda$, what happens to model bias and variance?
\begin{enumerate}[A)]
\item Both bias and variance increase
\item Both bias and variance decrease
\item Bias increases, variance decreases
\item Bias decreases, variance increases
\end{enumerate}
\end{popquizbox}
\end{frame}

\begin{frame}{Answer: Pop Quiz \thepopquiz}
\begin{popquizbox}{Answer: Pop Quiz \thepopquiz}
\textbf{C) Bias increases, variance decreases}

\vspace{0.3cm}
Explanation:
\begin{itemize}
\item Increasing $\lambda$ constrains coefficients more severely
\item Model becomes simpler (higher bias)
\item Less sensitive to training data variations (lower variance)
\item This is the fundamental bias-variance trade-off!
\end{itemize}
\end{popquizbox}
\end{frame}

\section{Examples and Applications}

\begin{frame}{Worked Example: Setup}
\begin{examplebox}{Ridge Regression Example}
Given the following simple dataset, compare OLS vs. Ridge regression with $\lambda = 2$:

\vspace{0.3cm}
Data: $(x_1, y_1) = (1, 1)$, $(x_2, y_2) = (2, 2)$, $(x_3, y_3) = (3, 3)$, $(x_4, y_4) = (4, 0)$

Model: $y = \theta_0 + \theta_1 x$
\end{examplebox}
\pause

\begin{codebox}{Step 1: Set up matrices}
$$\mX = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}, \quad \vy = \begin{bmatrix} 1 \\ 2 \\ 3 \\ 0 \end{bmatrix}, \quad \vtheta = \begin{bmatrix} \theta_0 \\ \theta_1 \end{bmatrix}$$
\end{codebox}
\end{frame}

\begin{frame}{Worked Example: OLS Setup}
\begin{codebox}{Step 2: Ordinary Least Squares}
$$\hat{\vtheta}_{\text{OLS}} = (\mX^{T}\mX)^{-1}(\mX^{T}\vy)$$
\end{codebox}
\pause

\begin{codebox}{Step 3: Compute matrix products}
\begin{align*}
\mX^{T}\mX &= \begin{bmatrix} 4 & 10 \\ 10 & 30 \end{bmatrix} \\
\mX^{T}\vy &= \begin{bmatrix} 6 \\ 14 \end{bmatrix}
\end{align*}
\end{codebox}
\end{frame}

\begin{frame}{Worked Example: Matrix Inverse}
\begin{codebox}{Step 4: Compute the inverse}
For $\mX^T\mX = \begin{bmatrix} 4 & 10 \\ 10 & 30 \end{bmatrix}$:

$\det(\mX^T\mX) = 4 \cdot 30 - 10 \cdot 10 = 20$

$$(\mX^{T}\mX)^{-1} = \frac{1}{20} \begin{bmatrix} 30 & -10 \\ -10 & 4 \end{bmatrix}$$
\end{codebox}
\end{frame}

\begin{frame}{Worked Example: OLS Calculation}
\begin{codebox}{Step 5: Final matrix multiplication}
\begin{align*}
\hat{\vtheta}_{\text{OLS}} &= (\mX^{T}\mX)^{-1}(\mX^{T}\vy) \\
&= \frac{1}{20} \begin{bmatrix} 30 & -10 \\ -10 & 4 \end{bmatrix} \begin{bmatrix} 6 \\ 14 \end{bmatrix} \\
&= \frac{1}{20} \begin{bmatrix} 180-140 \\ -60+56 \end{bmatrix} = \frac{1}{20} \begin{bmatrix} 40 \\ -4 \end{bmatrix} = \begin{bmatrix} 2.0 \\ -0.2 \end{bmatrix}
\end{align*}
\end{codebox}
\end{frame}

\begin{frame}{OLS Final Result}
\begin{theorembox}{OLS Result}
$\hat{y} = 2.0 - 0.2x$ \quad (No regularization)
\end{theorembox}

\begin{figure}
\centering
\begin{tikzpicture}[scale=0.8]
\begin{axis}[
    width=8cm, height=4.5cm,
    xlabel={$x$}, ylabel={$y$},
    xmin=0.5, xmax=4.5, ymin=0.5, ymax=4.5,
    grid=major, grid style={thin,gray!30},
    legend to name=legend1
]
% Data points
\addplot[only marks, mark=*, mark size=3pt, blue] coordinates {
    (1,1) (2,2) (3,3) (4,0)
};
% OLS line: y = 2.0 - 0.2x
\addplot[red, thick, domain=0.5:4.5] {2.0 - 0.2*x};
\legend{Data, OLS: $\hat{y} = 2.0 - 0.2x$}
\end{axis}
\end{tikzpicture}
\vspace{-0.2cm}
\ref{legend1}
\caption{{\small OLS fit to our example data}}
\end{figure}
\end{frame}

\begin{frame}{Worked Example: Ridge Setup}
\begin{codebox}{Step 5: Ridge regression with $\lambda = 2$}
$$\hat{\vtheta}_{\text{ridge}} = (\mX^{T}\mX + \lambda\mI)^{-1}(\mX^{T}\vy)$$
\end{codebox}
\pause

\begin{codebox}{Step 6: Add regularization term}
\begin{align*}
\mX^{T}\mX + \lambda\mI &= \begin{bmatrix} 4 & 10 \\ 10 & 30 \end{bmatrix} + 2\mI \\
&= \begin{bmatrix} 6 & 10 \\ 10 & 32 \end{bmatrix}
\end{align*}
\end{codebox}
\end{frame}

\begin{frame}{Worked Example: Matrix Inverse}
\begin{codebox}{Step 7: Compute inverse}
$\det(\mX^T\mX + \lambda\mI) = 6 \cdot 32 - 10 \cdot 10 = 92$

$$(\mX^{T}\mX + \lambda\mI)^{-1} = \frac{1}{92} \begin{bmatrix} 32 & -10 \\ -10 & 6 \end{bmatrix}$$
\end{codebox}
\end{frame}

\begin{frame}{Worked Example: Ridge Calculation}
\begin{codebox}{Step 8: Matrix multiplication}
\begin{align*}
\hat{\vtheta}_{\text{ridge}} &= (\mX^{T}\mX + \lambda\mI)^{-1}(\mX^{T}\vy) \\
&= \frac{1}{92} \begin{bmatrix} 32 & -10 \\ -10 & 6 \end{bmatrix} \begin{bmatrix} 6 \\ 14 \end{bmatrix}
\end{align*}
\end{codebox}
\pause

\begin{codebox}{Step 9: Compute products}
\begin{align*}
&= \frac{1}{92} \begin{bmatrix} 32 \cdot 6 + (-10) \cdot 14 \\ (-10) \cdot 6 + 6 \cdot 14 \end{bmatrix} \\
&= \frac{1}{92} \begin{bmatrix} 192-140 \\ -60+84 \end{bmatrix} = \frac{1}{92} \begin{bmatrix} 52 \\ 24 \end{bmatrix} = \begin{bmatrix} 0.565 \\ 0.261 \end{bmatrix}
\end{align*}
\end{codebox}
\end{frame}

\begin{frame}{Ridge vs OLS: Final Comparison}
\begin{theorembox}{Ridge Result}
$\hat{y} = 0.565 + 0.261x$ \quad (With $\lambda = 2$)
\end{theorembox}

\begin{figure}
\centering
\begin{tikzpicture}[scale=0.8]
\begin{axis}[
    width=8cm, height=4.5cm,
    xlabel={$x$}, ylabel={$y$},
    xmin=0.5, xmax=4.5, ymin=-0.5, ymax=3.5,
    grid=major, grid style={thin,gray!30},
    legend to name=legend2
]
% Data points
\addplot[only marks, mark=*, mark size=3pt, blue] coordinates {
    (1,1) (2,2) (3,3) (4,0)
};
% OLS line: y = 2.0 - 0.2x
\addplot[red, thick, domain=0.5:4.5] {2.0 - 0.2*x};
% Ridge line: y = 0.565 + 0.261x
\addplot[green!70!black, thick, domain=0.5:4.5] {0.565 + 0.261*x};
\legend{Data, OLS: $\hat{y} = 2.0 - 0.2x$, Ridge: $\hat{y} = 0.565 + 0.261x$}
\end{axis}
\end{tikzpicture}
\vspace{-0.3cm}
\ref{legend2}
\caption{{\small Ridge regression provides more stable coefficients}}
\end{figure}
\end{frame}

\begin{frame}{Coefficient Magnitude Comparison}
\begin{theorembox}{OLS vs Ridge: $\sum \theta_i^2$}
\begin{itemize}
\item \textbf{OLS}: $\vtheta_{OLS} = \begin{bmatrix} 2.0 \\ -0.2 \end{bmatrix}$ 
\item \textbf{Ridge}: $\vtheta_{Ridge} = \begin{bmatrix} 0.565 \\ 0.261 \end{bmatrix}$
\end{itemize}
\end{theorembox}
\pause

\begin{codebox}{Coefficient Norm Comparison}
\begin{align}
\|\vtheta_{OLS}\|_2^2 &= (2.0)^2 + (-0.2)^2 = 4.0 + 0.04 = 4.04 \\[0.3em]
\|\vtheta_{Ridge}\|_2^2 &= (0.565)^2 + (0.261)^2 = 0.319 + 0.068 = 0.387
\end{align}
\end{codebox}
\pause

\begin{alertbox}{Key Result}
Ridge regression achieved a {\bf 90.4\% reduction} in coefficient magnitude!
$$\frac{0.387}{4.04} = 0.096 \quad \text{(Ridge is 9.6\% of OLS magnitude)}$$
\end{alertbox}
\end{frame}

\begin{frame}{Multi-collinearity}
$(\mX^{T}\mX)^{-1}$ is not computable when $|\mX^{T}\mX|$ = 0.

This was a drawback of using linear regression

\begin{equation*}
\mX = \begin{bmatrix}
1 & 1& 2\\
1 & 2& 4\\
1 & 3& 6\\
\end{bmatrix}
\end{equation*}

The matrix $\mX$ is not full rank. 
\end{frame}

\begin{frame}{Ridge Solution to Multi-collinearity}
\begin{keypointsbox}{Ridge Advantage}
With ridge regression, we invert $\mX^{T}\mX + \mu \mI$ instead of $\mX^{T}\mX$
\end{keypointsbox}

\begin{equation*}
\mX^T\mX + \mu \mI = \begin{bmatrix}
3+\mu & 6& 12\\
6 & 14+\mu & 28\\
12 & 28& 56+\mu \\
\end{bmatrix}
\end{equation*}
\end{frame}

\begin{frame}{Why Ridge Fixes Singularity}
\begin{theorembox}{Key Result}
The matrix $\mX^T\mX + \mu \mI$ is always full rank for $\mu > 0$
\end{theorembox}
\pause

\begin{alertbox}{Another Interpretation}
Ridge regression = regularization = fixing singularity issues!
\end{alertbox}
\pause

\begin{keypointsbox}{Summary}
Ridge regression elegantly handles multi-collinearity problems!
\end{keypointsbox}
\end{frame}


\begin{frame}{Extension of the analytical model}
For ridge with no penalty on $\theta_0$
$$
\hat{\vtheta} = \left(\mX^T\mX+\mu \mI^*\right)^{-1}\mX^T\vy
$$
where, $$\mI^* = \begin{bmatrix}
    \color{red}0 & 0 & 0 & \dots  & 0 \\
    0 & 1& 0 & \dots  & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \dots  & 1
\end{bmatrix}$$
\end{frame}

\section{Implementation Details}

\begin{frame}{Ridge Regression via Gradient Descent}
\begin{theorembox}{Gradient Descent Update Rule}
Standard gradient descent step for ridge regression:
$$\vtheta^{(t+1)} = \vtheta^{(t)} - \alpha \nabla J(\vtheta^{(t)})$$
\end{theorembox}
\pause

\begin{codebox}{Ridge Gradient Computation}
\begin{align}
\nabla J(\vtheta) &= \nabla \left[\frac{1}{2}\|\vy-\mX\vtheta\|_2^2 + \frac{\lambda}{2}\|\vtheta\|_2^2\right] \\
&= -\mX^T(\vy-\mX\vtheta) + \lambda\vtheta \\
&= -\mX^T\vy + \mX^T\mX\vtheta + \lambda\vtheta
\end{align}
\end{codebox}
\end{frame}

\begin{frame}{Ridge vs OLS: Gradient Descent Updates}
\begin{theorembox}{Ridge Update (with shrinkage)}
$$\vtheta^{(t+1)} = \vtheta^{(t)} - \alpha(-\mX^T\vy + \mX^T\mX\vtheta^{(t)} + \lambda\vtheta^{(t)})$$
$$= (1 - \alpha\lambda)\vtheta^{(t)} - \alpha(-\mX^T\vy + \mX^T\mX\vtheta^{(t)})$$
\end{theorembox}
\pause

\begin{theorembox}{OLS Update (no shrinkage)}
$$\vtheta^{(t+1)} = \vtheta^{(t)} - \alpha(-\mX^T\vy + \mX^T\mX\vtheta^{(t)})$$
\end{theorembox}
\pause

\begin{keypointsbox}{Key Insight}
The $(1 - \alpha\lambda)$ factor \textbf{shrinks} coefficients at each step!
\end{keypointsbox}
\end{frame}

\begin{frame}{Summary: What We Learned}
\begin{keypointsbox}{Ridge Regression Key Points}
\begin{itemize}
\item \textbf{Problem}: Overfitting in linear regression with large coefficients
\item \textbf{Solution}: Add $L_2$ penalty $\lambda\|\vtheta\|_2^2$ to loss function
\item \textbf{Effect}: Shrinks coefficients, improves generalization
\item \textbf{Trade-off}: Higher bias, lower variance
\end{itemize}
\end{keypointsbox}
\end{frame}

\begin{frame}{Key Formula \& Next Steps}
\begin{theorembox}{Ridge Regression Solution}
$$\hat{\vtheta}_{\text{ridge}} = (\mX^T\mX + \lambda\mI)^{-1}\mX^T\vy$$
\end{theorembox}
\pause

\begin{alertbox}{Next Steps}
\begin{itemize}
\item Compare with Lasso regression ($L_1$ penalty)
\item Explore elastic net (combines $L_1$ and $L_2$)
\item Apply to real-world datasets
\end{itemize}
\end{alertbox}
\end{frame}


\end{document}
