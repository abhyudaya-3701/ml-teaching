\documentclass[usenames,dvipsnames]{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/styles/conventions}
\usepackage{tikz}
\usepackage{pdfpages}
\usetikzlibrary{arrows.meta, positioning, decorations.pathreplacing, calc}

% Pop quiz counter already defined in custom style

\newcommand{\E}{\mathbb{E}}

\title{Gradient Descent: The Foundation of Machine Learning Optimization}
\subtitle{From Taylor Series to Modern Deep Learning}
\date{\today}
\author{Nipun Batra and the teaching staff}
\institute{IIT Gandhinagar}

\begin{document}
  \maketitle

  \begin{frame}{Table of Contents}
    \tableofcontents
  \end{frame}

  \section{Mathematical Foundations}
  
  \begin{frame}{The Big Picture: Why Optimization Matters}
    \begin{itemize}[<+->]
        \item \textbf{Core ML Problem:} Find best parameters $\vtheta^*$ for our model
        \item \textbf{Examples everywhere:}
        \begin{itemize}
            \item Linear regression: Minimize $(y - \mX\vtheta)^2$
            \item Neural networks: Minimize classification/regression loss
            \item Logistic regression: Minimize cross-entropy loss
        \end{itemize}
        \item \textbf{Challenge:} Most ML problems have no closed-form solution
        \item \textbf{Solution:} Iterative optimization algorithms
    \end{itemize}
    
    \pause
    \begin{keypointsbox}
    Gradient descent is the workhorse of modern machine learning!
    \end{keypointsbox}
  \end{frame}

  \begin{frame}{Gradient Intuition: Climbing Mountains}
    \textbf{Imagine you're hiking in dense fog and want to reach the valley:}
    
    \begin{itemize}[<+->]
        \item You can only feel the slope beneath your feet
        \item \textbf{Strategy:} Always step in the steepest downhill direction
        \item \textbf{Gradient} = Direction of steepest \textcolor{red}{uphill} (ascent)
        \item \textbf{Negative gradient} = Direction of steepest \textcolor{blue}{downhill} (descent)
    \end{itemize}
    
    \pause
    \begin{center}
    \includegraphics[scale=0.8]{../../maths/assets/mathematical-ml/figures/contour-x_squared_plus_y_squared_quiver-with-gradient.pdf}
    \end{center}
    
    \pause
    \textbf{Mathematical definition:} $\nabla f(x, y) = \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix}$
  \end{frame}

  \begin{frame}{Geometric Intuition with Level Sets}
    \begin{center}
    \begin{tikzpicture}[scale=1.3]
      % Level sets (ellipses)
      \draw[blue, thick] (0,0) ellipse (2.2 and 1.3);
      \draw[blue, thick] (0,0) ellipse (1.6 and 1);
      \draw[blue, thick] (0,0) ellipse (1 and 0.6);

      % Gradient arrows (red, perpendicular to contours)
      \draw[-{Latex[scale=2]}, red, very thick] (1.2,0.45) -- (2,0.75);
      \draw[-{Latex[scale=2]}, red, very thick] (-1,0.8) -- (-1.6,1.3);
      \draw[-{Latex[scale=2]}, red, very thick] (0.8,-0.3) -- (1.3,-0.5);

      % Minimum point
      \fill[green] (0,0) circle (0.08);
      \node[green] at (0,-0.3) {\textbf{minimum}};

      % Labels
      \node[blue] at (-1.5,1.5) {level sets};
      \node[red] at (2.2,0.9) {$\nabla f$};
    \end{tikzpicture}
    \end{center}

    \begin{keypointsbox}
    Key insight: Gradient $\perp$ level sets, points toward steepest ascent
    \end{keypointsbox}
  \end{frame}

  \section{Taylor Series: The Mathematical Foundation}

  \begin{frame}{Why Taylor Series? The Key Insight}
    \begin{definitionbox}{The Core Idea}
    If we can't solve $\min f(\vx)$ exactly, let's approximate $f(\vx)$ locally and optimize that!
    \end{definitionbox}
    
    \pause
    \textbf{Taylor series expansion around point $\vx_0$:}
    \begin{align}
        f(\vx) &= f(\vx_0) + \nabla f(\vx_0)^T(\vx-\vx_0) + \frac{1}{2}(\vx-\vx_0)^T\nabla^2 f(\vx_0)(\vx-\vx_0) + \ldots
    \end{align}
    
    \pause
    \begin{itemize}[<+->]
        \item \textbf{Zero-order:} $f(\vx) \approx f(\vx_0)$ (just the function value)
        \item \textbf{First-order:} $f(\vx) \approx f(\vx_0) + \nabla f(\vx_0)^T(\vx-\vx_0)$ (linear approximation)
        \item \textbf{Second-order:} Includes curvature via Hessian $\nabla^2 f(\vx_0)$
    \end{itemize}
  \end{frame}

  \begin{frame}{Univariate Taylor: Visual Understanding}
    \begin{center}
    \begin{tikzpicture}[scale=1.4]
      % Original function (blue curve)
      \draw[blue, very thick, domain=-1.5:2.5, samples=100] 
        plot (\x, {0.3*(\x-0.5)*(\x-0.5) + 1});

      % Point of tangency
      \fill[black] (0.5,1) circle (0.06);
      \node at (0.5,0.7) {$x_0$};

      % Tangent line (red)
      \draw[red, very thick, domain=-0.8:1.8] 
        plot (\x, {0.3*(\x-0.5) + 1});

      % Step direction
      \draw[-{Latex[scale=1.5]}, red, thick] (0.5,1) -- (0.2,0.91);
      \node[red] at (0.1,0.85) {$-\alpha f'(x_0)$};

      % Labels
      \node[blue] at (-1,2.2) {\textbf{f(x)}};
      \node[red] at (1.5,1.6) {\textbf{tangent}};

      % Axes
      \draw[->] (-1.5,0) -- (2.5,0) node[right] {$x$};
      \draw[->] (0,0) -- (0,2.5) node[above] {$f$};
    \end{tikzpicture}
    \end{center}
  \end{frame}

  \begin{frame}{Adding Quadratic Term}
    \begin{center}
    \begin{tikzpicture}[scale=1.4]
      % Original function (blue curve)
      \draw[blue, very thick, domain=-1.5:2.5, samples=100] 
        plot (\x, {0.3*(\x-0.5)*(\x-0.5) + 1});

      % Point of tangency
      \fill[black] (0.5,1) circle (0.06);

      % Tangent line (red, dashed)
      \draw[red, thick, dashed, domain=-0.8:1.8] 
        plot (\x, {0.3*(\x-0.5) + 1});

      % Quadratic approximation (green)
      \draw[green, very thick, domain=-0.8:1.8, samples=50] 
        plot (\x, {0.3*(\x-0.5)*(\x-0.5) + 1});

      % Labels
      \node[blue] at (-1,2.2) {\textbf{f(x)}};
      \node[red] at (1.5,1.6) {\textbf{linear}};
      \node[green] at (-1,1.6) {\textbf{quadratic}};

      % Axes
      \draw[->] (-1.5,0) -- (2.5,0) node[right] {$x$};
      \draw[->] (0,0) -- (0,2.5) node[above] {$f$};
    \end{tikzpicture}
    \end{center}

    \begin{alertbox}{Key Insight}
    Higher-order terms give better approximations, but first-order is often sufficient for optimization!
    \end{alertbox}
  \end{frame}

  \begin{frame}{Taylor Series: Concrete Example}
    \textbf{Let's approximate} $f(x) = \cos(x)$ \textbf{around} $x_0 = 0$:
    
    \begin{itemize}[<+->]
        \item $f(0) = \cos(0) = 1$
        \item $f'(0) = -\sin(0) = 0$
        \item $f''(0) = -\cos(0) = -1$
        \item $f'''(0) = \sin(0) = 0$
        \item $f^{(4)}(0) = \cos(0) = 1$
    \end{itemize}
    
    \pause
    \textbf{Taylor approximations:}
    \begin{align}
        \text{0th order:} \quad &f(x) \approx 1\\
        \text{2nd order:} \quad &f(x) \approx 1 - \frac{x^2}{2}\\
        \text{4th order:} \quad &f(x) \approx 1 - \frac{x^2}{2} + \frac{x^4}{24}
    \end{align}
  \end{frame}

  \begin{frame}{Multivariate Taylor Series}
    \textbf{Extension to multiple variables:}
    \begin{align}
        f(\vx) &= f(\vx_0) + \nabla f(\vx_0)^T(\vx-\vx_0) + \frac{1}{2}(\vx-\vx_0)^T\nabla^2 f(\vx_0)(\vx-\vx_0) + \ldots
    \end{align}
    
    \pause
    \begin{center}
    \begin{tikzpicture}[scale=1.5]
      % Draw coordinate system
      \draw[-{Latex[scale=1.5]}] (-0.5,0) -- (3,0) node[right] {$\Delta x_1$};
      \draw[-{Latex[scale=1.5]}] (0,-0.5) -- (0,2.5) node[above] {$\Delta x_2$};

      % Draw delta x vector
      \draw[-{Latex[scale=2]}, red, very thick] (0,0) -- (2,1.5);
      \node[red] at (2.3,1.7) {$\Delta\vx$};

      % Draw gradient vector
      \draw[-{Latex[scale=2]}, blue, very thick] (0,0) -- (1.5,2);
      \node[blue] at (1.2,2.3) {$\nabla f(\vx_0)$};

      % Show dot product angle
      \draw[dashed] (0,0) -- (1.33,1) -- (2,1.5);
      \node at (0.8,0.3) {$\theta$};

      \node at (1,-0.8) {\textbf{Linear term:} $\nabla f(\vx_0)^T \Delta\vx = |\nabla f||\Delta\vx|\cos\theta$};
    \end{tikzpicture}
    \end{center}
  \end{frame}

  \begin{frame}{Visual: Multivariate Case with Level Sets}
    \begin{center}
    \begin{tikzpicture}[scale=1.3]
      % Level sets
      \draw[blue, thick] (0,0) ellipse (2.2 and 1.3);
      \draw[blue, thick] (0,0) ellipse (1.6 and 1);
      \draw[blue, thick] (0,0) ellipse (1 and 0.6);

      % Point x0
      \fill[black] (1.4,0.7) circle (0.06);
      \node at (1.7,0.7) {$\vx_0$};

      % Gradient vector (red)
      \draw[-{Latex[scale=1.5]}, red, very thick] (1.4,0.7) -- (2.2,1.1);
      \node[red] at (2.5,1.2) {$\nabla f(\vx_0)$};

      % Tangent plane indication
      \draw[green, thick, dashed] (0.8,0.4) -- (2.0,1.0);
      \node[green] at (0.5,0.3) {tangent plane};

      \node[blue] at (-1.5,1.5) {level sets};
    \end{tikzpicture}
    \end{center}

    \textbf{Key:} Gradient $\perp$ level sets, tangent plane $\perp$ gradient
  \end{frame}

  \section{From Taylor Series to Gradient Descent}

  \begin{frame}{The Derivation: From Theory to Algorithm}
    \textbf{Goal:} Find $\Delta \vx$ such that $f(\vx_0 + \Delta \vx) < f(\vx_0)$
    
    \pause
    \textbf{Using first-order Taylor approximation:}
    \begin{align}
        f(\vx_0 + \Delta \vx) &\approx f(\vx_0) + \nabla f(\vx_0)^T \Delta \vx
    \end{align}
    
    \pause
    \textbf{To minimize, we need:} $\nabla f(\vx_0)^T \Delta \vx < 0$
    
    \pause
    \begin{examplebox}{Vector Geometry Insight}
    For vectors $\mathbf{a}$ and $\mathbf{b}$: $\mathbf{a}^T\mathbf{b} = |\mathbf{a}||\mathbf{b}|\cos(\theta)$
    \\Minimum when: $\cos(\theta) = -1$ (opposite directions!)
    \end{examplebox}
    
    \pause
    \textbf{Optimal choice:} $\Delta \vx = -\alpha \nabla f(\vx_0)$ where $\alpha > 0$
    
    \pause
    \begin{definitionbox}{Gradient Descent Update Rule}
    $$\vx_{\text{new}} = \vx_{\text{old}} - \alpha \nabla f(\vx_{\text{old}})$$
    \end{definitionbox}
  \end{frame}

  \stepcounter{popquiz}
  \begin{frame}{Pop Quiz \#\thepopquiz: Taylor Series Understanding}
    \begin{popquizbox}{\thepopquiz}
    Given $f(x) = x^2 + 2$ and expansion point $x_0 = 2$:
    
    \textbf{Questions:}
    \begin{enumerate}
        \item What is $f(x_0)$?
        \item What is $f'(x_0)$?
        \item Write the first-order Taylor approximation
        \item If we take a step $\Delta x = -0.1 \cdot f'(x_0)$, what is our new $x$?
    \end{enumerate}
    \end{popquizbox}
  \end{frame}

  \begin{frame}{Pop Quiz \#\thepopquiz: Solutions}
    \begin{examplebox}{Solutions}
    Given $f(x) = x^2 + 2$ and $x_0 = 2$:
    \begin{enumerate}
        \item $f(2) = 4 + 2 = 6$
        \item $f'(x) = 2x$, so $f'(2) = 4$
        \item $f(x) \approx 6 + 4(x-2) = 4x - 2$
        \item $\Delta x = -0.1 \times 4 = -0.4$, so $x_{new} = 2 - 0.4 = 1.6$
    \end{enumerate}
    \end{examplebox}
  \end{frame}

  \section{Gradient Descent Algorithm}

  \begin{frame}{The Gradient Descent Algorithm}
    \begin{definitionbox}{Gradient Descent}
    An iterative first-order optimization algorithm for finding local minima of differentiable functions
    \end{definitionbox}
    
    \pause
    \textbf{Algorithm:}
    \begin{enumerate}[<+->]
        \item \textbf{Initialize:} $\vtheta_0$ (random or educated guess)
        \item \textbf{For} $t = 0, 1, 2, \ldots$ until convergence:
        \begin{itemize}
            \item Compute gradient: $\vg_t = \nabla f(\vtheta_t)$
            \item Update parameters: $\vtheta_{t+1} = \vtheta_t - \alpha \vg_t$
            \item Check convergence: $|\vg_t| < \epsilon$ or $|f(\vtheta_{t+1}) - f(\vtheta_t)| < \epsilon$
        \end{itemize}
    \end{enumerate}
    
    \pause
    \begin{keypointsbox}
    \textbf{Key Properties:}
    \begin{itemize}
        \item First-order method (uses gradients, not Hessians)
        \item Greedy/local search (follows steepest descent)
        \item Guaranteed to converge for convex functions
    \end{itemize}
    \end{keypointsbox}
  \end{frame}

  \begin{frame}{The Learning Rate: Your Step Size}
    \textbf{The learning rate} $\alpha$ \textbf{controls how big steps we take}
    
    \begin{center}
    \begin{tikzpicture}[scale=1.2]
      % Quadratic function
      \draw[blue, very thick, domain=-2.5:2.5, samples=100] 
        plot (\x, {0.3*\x*\x + 0.2});

      % Minimum
      \fill[green] (0,0.2) circle (0.05);
      \node[green] at (0,0) {min};

      % Small alpha (many small steps)
      \foreach \x in {-2,-1.8,-1.6,-1.4,-1.2,-1,-0.8,-0.6,-0.4,-0.2} {
        \fill[orange] (\x, {0.3*\x*\x + 0.2}) circle (0.03);
      }
      \node[orange] at (-2.5,1.6) {\small $\alpha$ too small};

      % Good alpha (reasonable steps)  
      \foreach \x in {1.5,1,0.5} {
        \fill[red] (\x, {0.3*\x*\x + 0.2}) circle (0.04);
      }
      \node[red] at (1.8,0.8) {\small $\alpha$ just right};

      % Large alpha (overshoot)
      \fill[purple] (2.2, {0.3*2.2*2.2 + 0.2}) circle (0.05);
      \fill[purple] (-2.2, {0.3*2.2*2.2 + 0.2}) circle (0.05);
      \draw[purple, thick, <->] (2.2, {0.3*2.2*2.2 + 0.2}) -- (-2.2, {0.3*2.2*2.2 + 0.2});
      \node[purple] at (2.5,2.2) {\small $\alpha$ too large};

      % Axes
      \draw[->] (-2.7,0) -- (2.7,0) node[right] {$\theta$};
      \draw[->] (0,0) -- (0,2.5) node[above] {$f(\theta)$};
    \end{tikzpicture}
    \end{center}
  \end{frame}

  \begin{frame}{Learning Rate: Too Small ($\alpha = 0.01$)}
    \textbf{Convergence is slow but stable}
    \begin{center}
    \includegraphics[scale=0.8]{../../maths/assets/mathematical-ml/figures/gd-lr-0.01.pdf}
    \end{center}
    
    \begin{alertbox}{Problem}
    Takes many iterations to reach the minimum. Computationally expensive!
    \end{alertbox}
  \end{frame}

  \begin{frame}{Learning Rate: Just Right ($\alpha = 0.1$)}
    \textbf{Good balance: Fast and stable convergence}
    \begin{center}
    \includegraphics[scale=0.8]{../../maths/assets/mathematical-ml/figures/gd-lr-0.1.pdf}
    \end{center}
    
    \begin{keypointsbox}
    This is often the sweet spot for many problems!
    \end{keypointsbox}
  \end{frame}

  \begin{frame}{Learning Rate: Too Large ($\alpha = 0.8$)}
    \textbf{Fast but may overshoot}
    \begin{center}
    \includegraphics[scale=0.8]{../../maths/assets/mathematical-ml/figures/gd-lr-0.8.pdf}
    \end{center}
    
    \begin{alertbox}{Warning}
    Quick convergence but risk of instability. Watch out for oscillations!
    \end{alertbox}
  \end{frame}

  \begin{frame}{Learning Rate: Disaster ($\alpha = 1.01$)}
    \textbf{Divergence! Function values explode}
    \begin{center}
    \includegraphics[scale=0.8]{../../maths/assets/mathematical-ml/figures/gd-lr-1.01.pdf}
    \end{center}
    
    \begin{alertbox}{Disaster Zone}
    The algorithm diverges. Always monitor your loss curves!
    \end{alertbox}
  \end{frame}

  \section{Gradient Descent for Linear Regression}

  \begin{frame}{Linear Regression: Our First Real Application}
    \textbf{Problem:} Learn $y = \theta_0 + \theta_1 x$ from data
    
    \begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{x} & \textbf{y} \\
        \hline
        1 & 1 \\
        2 & 2 \\
        3 & 3 \\
        \hline
    \end{tabular}
    \end{center}
    
    \pause
    \textbf{Cost Function (Mean Squared Error):}
    $$\MSE(\theta_0, \theta_1) = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n}\sum_{i=1}^n (y_i - \theta_0 - \theta_1 x_i)^2$$
    
    \pause
    \textbf{Goal:} $(\theta_0^*, \theta_1^*) = \argmin_{\theta_0, \theta_1} \MSE(\theta_0, \theta_1)$
  \end{frame}

  \begin{frame}{Computing Gradients for Linear Regression}
    \textbf{We need:} $\nabla \MSE = \begin{bmatrix} \frac{\partial \MSE}{\partial \theta_0} \\ \frac{\partial \MSE}{\partial \theta_1} \end{bmatrix}$
    
    \pause
    \textbf{Let's compute each partial derivative:}
    
    \begin{align}
        \frac{\partial \MSE}{\partial \theta_0} &= \frac{2}{n}\sum_{i=1}^n (y_i - \theta_0 - \theta_1 x_i)(-1)\\
        &= -\frac{2}{n}\sum_{i=1}^n \epsilon_i
    \end{align}
    
    \pause
    \begin{align}
        \frac{\partial \MSE}{\partial \theta_1} &= \frac{2}{n}\sum_{i=1}^n (y_i - \theta_0 - \theta_1 x_i)(-x_i)\\
        &= -\frac{2}{n}\sum_{i=1}^n \epsilon_i x_i
    \end{align}
    
    where $\epsilon_i = y_i - \hat{y}_i$ is the residual.
  \end{frame}

  \begin{frame}{Gradient Descent: Step-by-Step Example}
    \textbf{Initial values:} $\theta_0 = 4, \theta_1 = 0$, \textbf{Learning rate:} $\alpha = 0.1$
    
    \pause
    \textbf{Iteration 1:}
    \begin{itemize}[<+->]
        \item Predictions: $\hat{y}_1 = 4, \hat{y}_2 = 4, \hat{y}_3 = 4$
        \item Errors: $\epsilon_1 = 1-4 = -3, \epsilon_2 = 2-4 = -2, \epsilon_3 = 3-4 = -1$
        \item $\frac{\partial \MSE}{\partial \theta_0} = -\frac{2}{3}(-3-2-1) = 4$
        \item $\frac{\partial \MSE}{\partial \theta_1} = -\frac{2}{3}(-3 \cdot 1 - 2 \cdot 2 - 1 \cdot 3) = 6.67$
        \item $\theta_0 = 4 - 0.1 \times 4 = 3.6$
        \item $\theta_1 = 0 - 0.1 \times 6.67 = -0.67$
    \end{itemize}
    
    \pause
    \textbf{New parameters:} $(\theta_0, \theta_1) = (3.6, -0.67)$
  \end{frame}

  \begin{frame}{Visual Journey: Gradient Descent in Action}
    \textbf{Let's watch gradient descent navigate the loss landscape:}
    
    \begin{center}
    \only<1>{\includegraphics[scale=0.7]{../../maths/assets/mathematical-ml/figures/gradient-descent-0.pdf}}
    \only<2>{\includegraphics[scale=0.7]{../../maths/assets/mathematical-ml/figures/gradient-descent-5.pdf}}
    \only<3>{\includegraphics[scale=0.7]{../../maths/assets/mathematical-ml/figures/gradient-descent-10.pdf}}
    \only<4>{\includegraphics[scale=0.7]{../../maths/assets/mathematical-ml/figures/gradient-descent-15.pdf}}
    \only<5>{\includegraphics[scale=0.7]{../../maths/assets/mathematical-ml/figures/gradient-descent-20.pdf}}
    \end{center}
    
    \textbf{Notice:} The algorithm takes larger steps when gradient is large, smaller steps as it approaches the minimum!
  \end{frame}

  \begin{frame}{Visual: GD Path on Loss Surface (TikZ Version)}
    \begin{center}
    \begin{tikzpicture}[scale=1.3]
      % Loss contours (elliptical)
      \draw[blue, thick] (0,0) ellipse (2.5 and 1.5);
      \draw[blue, thick] (0,0) ellipse (1.8 and 1.1);
      \draw[blue, thick] (0,0) ellipse (1.1 and 0.65);
      \draw[blue, thick] (0,0) ellipse (0.6 and 0.35);

      % Minimum
      \fill[green] (0,0) circle (0.06);
      \node[green] at (0,-0.3) {$\vtheta^*$};

      % GD path (red arrows showing steps)
      \coordinate (p1) at (2,1.2);
      \coordinate (p2) at (1.4,0.7);
      \coordinate (p3) at (0.8,0.3);
      \coordinate (p4) at (0.3,0.1);
      \coordinate (p5) at (0.1,0.03);

      \fill[red] (p1) circle (0.05);
      \draw[-{Latex[scale=1.5]}, red, thick] (p1) -- (p2);
      \draw[-{Latex[scale=1.5]}, red, thick] (p2) -- (p3);
      \draw[-{Latex[scale=1.5]}, red, thick] (p3) -- (p4);
      \draw[-{Latex[scale=1.5]}, red, thick] (p4) -- (p5);

      \node at (2.2,1.4) {$\vtheta_0$};
      \node[blue] at (-1.8,1.2) {loss contours};

      % Axes labels
      \node at (2.8,0) {$\theta_1$};
      \node at (0,1.8) {$\theta_0$};

      % Show step sizes getting smaller
      \node[red] at (1.7,0.9) {\scriptsize large steps};
      \node[red] at (0.6,0.15) {\scriptsize small steps};
    \end{tikzpicture}
    \end{center}

    \textbf{Notice:} Algorithm takes larger steps when gradient is large!
  \end{frame}

  \section{Variants of Gradient Descent}

  \begin{frame}{The Gradient Descent Family}
    \textbf{Three main variants based on how much data we use per update:}
    
    \begin{definitionbox}{Batch Gradient Descent (GD)}
    Use all training data to compute each gradient
    \end{definitionbox}
    
    \begin{definitionbox}{Stochastic Gradient Descent (SGD)}
    Use one sample to compute each gradient
    \end{definitionbox}
    
    \begin{definitionbox}{Mini-batch Gradient Descent (MBGD)}
    Use a small batch of samples to compute each gradient
    \end{definitionbox}
    
    \pause
    \textbf{Trade-offs:} Computational cost vs. convergence stability vs. memory usage
  \end{frame}

  \begin{frame}{Batch vs Stochastic vs Mini-batch}
    \begin{center}
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Method} & \textbf{Data per update} & \textbf{Updates per epoch} & \textbf{Convergence} \\
        \hline
        Batch GD & $n$ (all) & 1 & Smooth \\
        \hline
        SGD & 1 & $n$ & Noisy \\
        \hline
        Mini-batch GD & $b$ (batch size) & $n/b$ & Balanced \\
        \hline
    \end{tabular}
    \end{center}
    
    \pause
    \begin{keypointsbox}
    \textbf{Modern ML:} Mini-batch GD with batch sizes 32-256 is most common
    \begin{itemize}
        \item Good balance of stability and efficiency
        \item Enables parallel computation (GPUs love batches!)
        \item Better gradient estimates than pure SGD
    \end{itemize}
    \end{keypointsbox}
  \end{frame}

  \begin{frame}{Stochastic Gradient Descent: The Noisy Path}
    \textbf{SGD uses one sample at a time for updates}
    
    \begin{center}
    \includegraphics[scale=0.5]{../../maths/assets/mathematical-ml/figures/gradient-descent-3-functions.pdf}
    \end{center}
    
    \pause
    \begin{itemize}[<+->]
        \item \textbf{Pro:} Fast updates, can escape local minima due to noise
        \item \textbf{Con:} Noisy convergence, may never reach exact minimum
        \item \textbf{Key insight:} The noise can be beneficial for non-convex problems!
    \end{itemize}
  \end{frame}

  \begin{frame}{Epochs vs Iterations: Important Distinction}
    \begin{definitionbox}{Iteration}
    One parameter update step (one gradient computation and update)
    \end{definitionbox}
    
    \begin{definitionbox}{Epoch}
    One complete pass through the entire training dataset
    \end{definitionbox}
    
    \pause
    \textbf{For dataset with 1000 samples:}
    \begin{itemize}[<+->]
        \item \textbf{Batch GD:} 1 iteration = 1 epoch
        \item \textbf{SGD:} 1000 iterations = 1 epoch  
        \item \textbf{Mini-batch (batch size 100):} 10 iterations = 1 epoch
    \end{itemize}
    
    \pause
    \begin{alertbox}{Important}
    Always report which metric you're using when discussing convergence speed!
    \end{alertbox}
  \end{frame}

  \section{Mathematical Properties}

  \begin{frame}{SGD as an Unbiased Estimator}
\textbf{True gradient:}
$\nabla L(\vtheta)=\tfrac{1}{n}\sum_{i=1}^n \nabla \ell(f(\vx_i;\vtheta),y_i)$

\pause
\textbf{SGD gradient estimate:}
$\nabla \tilde{L}(\vtheta)=\nabla \ell(f(\vx;\vtheta),y)$,
where $(\vx,y)$ is sampled uniformly from $\{(\vx_i,y_i)\}_{i=1}^n$.

\pause
\begin{theorembox}{Unbiased Estimator Property}
\[
\E[\nabla \tilde{L}(\vtheta)] = \nabla L(\vtheta)
\]
\end{theorembox}

\pause
\textbf{Proof:}
\begin{align*}
\E[\nabla \tilde{L}(\vtheta)]
&=\E\big[\nabla \ell(f(\vx;\vtheta),y)\big]\\
&=\sum_{i=1}^n \tfrac{1}{n}\,\nabla \ell(f(\vx_i;\vtheta),y_i)
= \nabla L(\vtheta).
\end{align*}
  \end{frame}

  \begin{frame}{Why Unbiasedness Matters}
    \begin{keypointsbox}
    \textbf{Unbiased means:} On average, SGD points in the right direction!
    \end{keypointsbox}
    
    \pause
    \textbf{Implications:}
    \begin{itemize}[<+->]
        \item Individual SGD steps might be ``wrong'', but they average to the correct direction
        \item This theoretical guarantee justifies why SGD works in practice
        \item The noise in SGD can actually help escape local minima
    \end{itemize}
    
    \pause
    \begin{examplebox}{Intuitive Analogy}
    Imagine asking random people for directions to a destination:
    \begin{itemize}
        \item Individual answers might be slightly off
        \item But if there's no systematic bias, the average direction is correct
        \item SGD does the same with gradient estimates!
    \end{itemize}
    \end{examplebox}
  \end{frame}

  \section{Computational Complexity}

  \begin{frame}{Computational Complexity: GD vs Normal Equation}
    \textbf{For linear regression, we have two options:}
    
    \begin{alertbox}{Normal Equation}
    $\hat{\vtheta} = (\mX^T\mX)^{-1}\mX^T\vy$
    \\Time complexity: $\mathcal{O}(d^2n + d^3)$
    \end{alertbox}
    
    \begin{keypointsbox}{Gradient Descent}
    $\vtheta_{t+1} = \vtheta_t - \alpha \mX^T(\mX\vtheta_t - \vy)$
    \\Time complexity: $\mathcal{O}(T \cdot dn)$ for $T$ iterations
    \end{keypointsbox}
    
    \pause
    \textbf{When to use which?}
    \begin{itemize}[<+->]
        \item \textbf{Few features ($d$ small):} Normal equation
        \item \textbf{Many features ($d$ large):} Gradient descent
        \item \textbf{Non-linear models:} Only gradient descent works
    \end{itemize}
  \end{frame}

  \begin{frame}{Complexity Analysis: Breaking It Down}
    \textbf{Gradient Descent per iteration:}
    \begin{itemize}[<+->]
        \item Compute $\mX\vtheta$: $\mathcal{O}(nd)$ 
        \item Compute residual $\mX\vtheta - \vy$: $\mathcal{O}(n)$
        \item Compute $\mX^T$ (residual): $\mathcal{O}(nd)$
        \item Update $\vtheta$: $\mathcal{O}(d)$
        \item \textbf{Total per iteration:} $\mathcal{O}(nd)$
    \end{itemize}
    
    \pause
    \textbf{Normal Equation (one-time):}
    \begin{itemize}[<+->]
        \item Compute $\mX^T\mX$: $\mathcal{O}(d^2n)$
        \item Invert $\mX^T\mX$: $\mathcal{O}(d^3)$
        \item Compute $\mX^T\vy$: $\mathcal{O}(dn)$
        \item Final multiplication: $\mathcal{O}(d^2)$
        \item \textbf{Total:} $\mathcal{O}(d^2n + d^3)$
    \end{itemize}
  \end{frame}

  \stepcounter{popquiz}
  \begin{frame}{Pop Quiz \#\thepopquiz: Complexity Comparison}
    \begin{popquizbox}{\thepopquiz}
    You have a dataset with $n = 10^6$ samples and $d = 10^3$ features.
    
    \textbf{Questions:}
    \begin{enumerate}
        \item What's the complexity of normal equation?
        \item What's the complexity of 100 GD iterations?
        \item Which method would you choose and why?
        \item What if $d = 10^6$ instead?
    \end{enumerate}
    \end{popquizbox}
  \end{frame}

  \begin{frame}{Pop Quiz \#\thepopquiz: Solutions}
    \begin{examplebox}{Solutions}
    For $n = 10^6$, $d = 10^3$:
    \begin{enumerate}
        \item Normal equation: $O(d^2n + d^3) = O(10^{12} + 10^9) = O(10^{12})$
        \item 100 GD iterations: $O(100 \cdot dn) = O(10^{11})$
        \item Choose GD (10× faster)
        \item If $d = 10^6$: Normal equation becomes $O(10^{18})$, GD becomes $O(10^{14})$ – definitely choose GD!
    \end{enumerate}
    \end{examplebox}
  \end{frame}

  \section{Advanced Topics and Extensions}

  \begin{frame}{Beyond Basic Gradient Descent}
    \textbf{Modern deep learning uses advanced optimizers:}
    
    \begin{itemize}[<+->]
        \item \textbf{Momentum:} $\vv_{t+1} = \beta \vv_t + (1-\beta)\vg_t$
        \item \textbf{AdaGrad:} Adaptive learning rates per parameter
        \item \textbf{Adam:} Combines momentum + adaptive learning rates  
        \item \textbf{RMSprop:} Exponential moving average of squared gradients
    \end{itemize}
    
    \pause
    \begin{examplebox}{Why These Improvements?}
    \begin{itemize}
        \item Handle different parameter scales automatically
        \item Accelerate convergence in relevant directions
        \item Reduce oscillations in narrow valleys
        \item Better performance on non-convex landscapes
    \end{itemize}
    \end{examplebox}
  \end{frame}

  \begin{frame}{Gradient Descent in Deep Learning}
    \begin{keypointsbox}
    Every modern deep learning framework uses gradient descent variants!
    \end{keypointsbox}
    
    \pause
    \textbf{Key extensions:}
    \begin{itemize}[<+->]
        \item \textbf{Backpropagation:} Efficient gradient computation for neural networks
        \item \textbf{Automatic differentiation:} PyTorch, TensorFlow handle gradients automatically
        \item \textbf{GPU acceleration:} Parallel computation of mini-batch gradients
        \item \textbf{Mixed precision:} Use both 16-bit and 32-bit arithmetic
    \end{itemize}
  \end{frame}

  \section{Practical Considerations}

  \begin{frame}{Choosing Learning Rates: Practical Tips}
    \begin{keypointsbox}
    Learning rate selection is more art than science!
    \end{keypointsbox}
    
    \pause
    \textbf{Common strategies:}
    \begin{itemize}[<+->]
        \item \textbf{Grid search:} Try $\{0.001, 0.01, 0.1, 1.0\}$
        \item \textbf{Learning rate schedules:} Start high, decay over time
        \item \textbf{Adaptive methods:} Let the algorithm adjust automatically
        \item \textbf{Learning rate finder:} Gradually increase $\alpha$ and watch loss
    \end{itemize}
    
    \pause
    \begin{alertbox}{Warning Signs}
    \begin{itemize}
        \item Loss exploding $\rightarrow$ Learning rate too high
        \item Very slow convergence $\rightarrow$ Learning rate too low  
        \item Oscillating loss $\rightarrow$ Try smaller learning rate or momentum
    \end{itemize}
    \end{alertbox}
  \end{frame}

  \begin{frame}{Convergence Criteria: When to Stop?}
    \textbf{Common stopping criteria:}
    
    \begin{itemize}[<+->]
        \item \textbf{Gradient magnitude:} $||\nabla f(\vtheta)|| < \epsilon$
        \item \textbf{Function value change:} $|f(\vtheta_{t+1}) - f(\vtheta_t)| < \epsilon$
        \item \textbf{Parameter change:} $||\vtheta_{t+1} - \vtheta_t|| < \epsilon$
        \item \textbf{Maximum iterations:} Simple upper bound
    \end{itemize}
    
    \pause
    \begin{examplebox}{Practical Advice}
    \begin{itemize}
        \item Always set a maximum iteration limit
        \item Monitor multiple criteria simultaneously  
        \item Use validation set performance in practice
        \item Early stopping prevents overfitting
    \end{itemize}
    \end{examplebox}
  \end{frame}

  \begin{frame}{Common Pitfalls and How to Avoid Them}
    \begin{alertbox}{Pitfall 1: Poor Initialization}
    \textbf{Problem:} Starting at bad points (e.g., all zeros)
    \\\textbf{Solution:} Use Xavier/He initialization for neural networks
    \end{alertbox}
    
    \pause
    \begin{alertbox}{Pitfall 2: Learning Rate Too High/Low}
    \textbf{Problem:} Divergence or slow convergence
    \\\textbf{Solution:} Learning rate schedules, grid search, or adaptive optimizers
    \end{alertbox}
    
    \pause
    \begin{alertbox}{Pitfall 3: Poor Feature Scaling}
    \textbf{Problem:} Different parameter scales cause poor convergence
    \\\textbf{Solution:} Standardize features: $(x - \mu)/\sigma$
    \end{alertbox}
  \end{frame}

  \section{Summary and Key Takeaways}

  \begin{frame}{What We've Learned: The Big Picture}
    \begin{keypointsbox}
    Gradient descent is the foundation of modern machine learning optimization!
    \end{keypointsbox}
    
    \pause
    \textbf{Core concepts:}
    \begin{itemize}[<+->]
        \item \textbf{Mathematical foundation:} Taylor series approximation
        \item \textbf{Geometric intuition:} Follow steepest descent direction
        \item \textbf{Algorithm variants:} Batch, SGD, mini-batch
        \item \textbf{Theoretical properties:} SGD is unbiased estimator
        \item \textbf{Practical considerations:} Learning rates, convergence criteria
    \end{itemize}
  \end{frame}

  \begin{frame}{From Theory to Practice: Next Steps}
    \textbf{Practice opportunities:}
    \begin{itemize}[<+->]
        \item Implement gradient descent from scratch
        \item Experiment with different learning rates
        \item Try different optimization functions
        \item Compare batch vs SGD vs mini-batch
        \item Visualize convergence paths
    \end{itemize}
    
    \pause
    \begin{keypointsbox}
    Master gradient descent first - it's the building block for everything else!
    \end{keypointsbox}
  \end{frame}

  \stepcounter{popquiz}
  \begin{frame}{Pop Quiz \#\thepopquiz: Comprehensive Review}
    \begin{popquizbox}{\thepopquiz}
    \textbf{True or False?}
    \begin{enumerate}
        \item SGD always converges faster than batch gradient descent
        \item The learning rate should decrease as training progresses
        \item SGD gradient estimates are unbiased
        \item Normal equation is always better than gradient descent
        \item Gradient descent can only find global minima
    \end{enumerate}
    \end{popquizbox}
  \end{frame}

  \begin{frame}{Pop Quiz \#\thepopquiz: Solutions}
    \begin{examplebox}{Solutions}
    \begin{enumerate}
        \item \textbf{False} - SGD converges faster per epoch but may need more epochs
        \item \textbf{True} - Learning rate schedules often improve convergence
        \item \textbf{True} - This is the key theoretical property of SGD
        \item \textbf{False} - Normal equation only works for linear problems and small $d$
        \item \textbf{False} - GD finds local minima; global minima only guaranteed for convex functions
    \end{enumerate}
    \end{examplebox}
  \end{frame}

  \begin{frame}{Looking Ahead: Advanced Optimization}
    \textbf{What's next in optimization?}
    
    \begin{itemize}[<+->]
        \item \textbf{Second-order methods:} Newton's method, L-BFGS
        \item \textbf{Constrained optimization:} Lagrange multipliers, KKT conditions  
        \item \textbf{Global optimization:} Simulated annealing, genetic algorithms
        \item \textbf{Distributed optimization:} Federated learning, parameter servers
        \item \textbf{Meta-learning:} Learning to optimize
    \end{itemize}
  \end{frame}

  % Include SGD notes
  \begin{frame}{Additional Resources: SGD Deep Dive}
    \begin{center}
    \textbf{For detailed mathematical analysis and proofs:}
    \end{center}
    
    % Include the SGD.pdf as reference
    \begin{alertbox}{Reference Material}
    See SGD.pdf in the assets folder for:
    \begin{itemize}
        \item Formal convergence proofs
        \item Variance analysis of SGD
        \item Advanced theoretical properties
        \item Comparison with other optimization methods
    \end{itemize}
    \end{alertbox}
  \end{frame}

  % Include SGD.pdf pages
  \includepdf[pages=1-5,fitpaper]{../assets/SGD.pdf}

  \begin{frame}
    \centering
    \Huge \textbf{Thank You!}
    
    \vspace{1cm}
    \Large Questions?
    
    \vspace{1cm}
    \normalsize
    \textbf{Next:} Advanced Optimization Techniques
    
    \textbf{Practice:} Implement gradient descent for your favorite ML model!
  \end{frame}

\end{document}